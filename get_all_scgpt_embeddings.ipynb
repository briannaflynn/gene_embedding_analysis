{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0658b4e2-e829-4e32-83e3-0c2283be5c70",
   "metadata": {},
   "source": [
    "# scGPT Embeddings Download and Set-Up Documentation\n",
    "\n",
    "This guide should help you get scGPT working with flash-attn (on Lambda or other cloud service) while avoiding the pitfalls I encountered.\n",
    "\n",
    "---\n",
    "### 1. Setting Up the Instance\n",
    "\n",
    "1. Download and install Anaconda:\n",
    "```\n",
    "wget <https://repo.anaconda.com/archive/Anaconda3-2024.06-1-Linux-x86_64.sh> \n",
    "bash Anaconda3-2024.06-1-Linux-x86_64.sh source ~/.bashrc\n",
    "```\n",
    "\n",
    "2. Create and activate the scGPT Conda environment:\n",
    "\n",
    "```conda create -n \"scgpt_conda\" python=3.9 conda activate scgpt_conda```\n",
    "\n",
    "3. Install Jupyter kernel:\n",
    "\n",
    "```conda install ipykernel python -m ipykernel install --user --name scgpt_conda --display-name \"scgpt_conda 3.9\"```\n",
    "\n",
    "4. Install necessary Python packages:\n",
    "\n",
    "```bash\n",
    "pip install scgpt \n",
    "pip install gseapy\n",
    "pip install gdown\n",
    "pip install PyMuPdf\n",
    "pip install fitz\n",
    "```\n",
    "\n",
    "Note: Installing scGPT without flash attention is fine when not training.\n",
    "\n",
    "5. Download required models using gdown:\n",
    "\n",
    "```gdown --folder <https://drive.google.com/drive/folders/1kkug5C7NjvXIwQGGaGoqXTk_Lb_pDrBU>```\n",
    "\n",
    " `Additional model downloads:`\n",
    "\n",
    "```\n",
    "gdown --folder <https://drive.google.com/drive/folders/1_GROJTzXiAV8HB4imruOTk6PEGuNOcgB> # CP gdown --folder <https://drive.google.com/drive/folders/1vf1ijfQSk7rGdDGpBntR5bi5g6gNt-Gx> # Brain gdown --folder <https://drive.google.com/drive/folders/1kkug5C7NjvXIwQGGaGoqXTk_Lb_pDrBU> # BC gdown --folder <https://drive.google.com/drive/folders/1GcgXrd7apn6y4Ze_iSCncskX3UsWPY2r> # Heart gdown --folder <https://drive.google.com/drive/folders/16A1DJ30PT6bodt4bWLa4hpS7gbWZQFBG> # Lung gdown --folder <https://drive.google.com/drive/folders/1S-1AR65DF120kNFpEbWCvRHPhpkGK3kK> # Kidney gdown --folder <http://drive.google.com/drive/folders/13QzLHilYUd0v3HTwa_9n4G4yEF-hdkqa> # Pan Cancer gdown --folder <https://drive.google.com/drive/folders/1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y> # Human (all 33 million)\n",
    "```\n",
    "\n",
    " 6. Clone the gene embedding analysis repository:\n",
    "\n",
    "```git clone <https://github.com/briannaflynn/gene_embedding_analysis.git>```\n",
    "\n",
    "### 2. Installing Dependencies\n",
    "\n",
    "1\\. Update package lists and install build tools:\n",
    "\n",
    "```sudo apt-get update sudo apt-get install build-essential ninja-build```\n",
    "\n",
    "2. Install Python development headers and libraries:\n",
    "\n",
    "```sudo apt-get install python3-dev```\n",
    "\n",
    "### 3. Verifying Compiler Setup\n",
    "\n",
    "1. Create a test program file named `test_pybind11.cpp`:\n",
    "\n",
    "#include <pybind11/pybind11.h> int main() { return 0; }\n",
    "\n",
    "2. Compile the test program to check if the compiler can find headers:\n",
    "\n",
    "```g++ test_pybind11.cpp -o test_pybind11 -I${CONDA_PREFIX}/include/python3.8 -L${CONDA_PREFIX}/lib -lpython3.8```\n",
    "\n",
    "### 4. Setting Environment Variables\n",
    "\n",
    "Set necessary environment variables to ensure correct library linking:\n",
    "\n",
    "```bash\n",
    "export CPLUS_INCLUDE_PATH=${CONDA_PREFIX}/include:${CONDA_PREFIX}/include/python3.8:$CPLUS_INCLUDE_PATH export C_INCLUDE_PATH=${CONDA_PREFIX}/include:${CONDA_PREFIX}/include/python3.8:$C_INCLUDE_PATH export LIBRARY_PATH=${CONDA_PREFIX}/lib:$LIBRARY_PATH export LD_LIBRARY_PATH=${CONDA_PREFIX}/lib:$LD_LIBRARY_PATH\n",
    "```\n",
    "\n",
    "### 5. Ensuring pip Uses the Correct Environment\n",
    "\n",
    "#### Issue:\n",
    "When using Lambda, pip may still point to the base user Python installation instead of the Conda environment.\n",
    "\n",
    "#### Fix:\n",
    "1. Add the following to `~/.bashrc`:\n",
    "\n",
    "```export PATH=\"/home/ubuntu/anaconda3/envs/scgpt_conda/bin:$PATH\"```\n",
    "\n",
    "2. Apply the changes:\n",
    "\n",
    "```source ~/.bashrc```\n",
    "\n",
    "3. Verify the correct pip path:\n",
    "\n",
    "```which pip```\n",
    "\n",
    "`Expected output:`\n",
    "\n",
    "```/home/ubuntu/anaconda3/envs/scgpt_conda/bin/pip```\n",
    "\n",
    "### 6. Installing Flash Attention\n",
    "\n",
    "The original instructions reference `flash-attn<1.0.5`, but this conflicts with CUDA 12. Instead, install version 1.0.6:\n",
    "\n",
    "```pip install \"flash-attn==1.0.6\" --no-build-isolation```\n",
    "\n",
    " `- `--no-build-isolation`: Ensures that the package is built using the current environment's installed packages instead of an isolated build.\n",
    "- If this causes issues in the future, downgrading to CUDA 11.7 may be necessary.\n",
    "\n",
    "---\n",
    "### 7. Installing Missing Dependencies\n",
    "\n",
    "1. Install `wandb` for logging and visualization:`\n",
    "\n",
    "```pip install wandb```\n",
    "\n",
    "### 8. Issues with scvi-tools and optax (JAX)\n",
    "\n",
    "The documentation states that scGPT supports Python 3.8+, but certain dependencies required for fine-tuning need **Python 3.9**.\n",
    "\n",
    "To resolve this:\n",
    "- Set up a clean Python 3.9 environment.\n",
    "- Reinstall scGPT and dependencies.\n",
    "- Verify the exact installation versions used in the scGPT Docker branch (see the relevant pull request for details).\n",
    "\n",
    "---\n",
    "\n",
    "### 9. There is an issue with numpy 2.0 compatibility, must downgrade\n",
    "\n",
    "```pip install \"numpy<2.0\"```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816cb9d7-2a1e-41ca-ac08-3e2437ba4b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f06227-1f94-4fb2-a59f-fb82dc470b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# needs to be version 1, not version 2\n",
    "import numpy\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab7a434-f3f7-4d62-a5c1-333cf4de2f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/anaconda3/envs/scgpt_conda/bin/python'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68456e6b-7197-4042-a3d0-a436d7c97aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed8ed818-b015-4831-861a-1f7c4db53ce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scgpt; # silencing the warning about flash attention\n",
    "# - kind of a pain to install and won't need it unless training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0af28d-27b8-40a3-a17b-c22dcbdd31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43ae5f45-b986-464b-93cd-6bce8cbba2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d67ef585-1d85-477e-b7a2-cba936bcf588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18f49317-36a1-4b49-84d3-92d2b15ccdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gseapy as gp\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d1ad3c7-473c-417c-837a-2710de38b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.tasks import GeneEmbedding\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0bdd28d3-7615-4f63-a35a-b10382a5f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export2embeddings(model_name=\"./scGPT_bc\", output_name=\"genes_names_embeddings.pkl\"):\n",
    "\n",
    "    # configs\n",
    "    # setting parameters, seed value, num highly variable genes, num bins, etc\n",
    "    set_seed(42)\n",
    "    pad_token = \"<pad>\"\n",
    "    special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "    n_hvg = 1200\n",
    "    n_bins = 51\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "    # load model\n",
    "    model_dir = Path(model_name)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "    special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "\n",
    "    for s in special_tokens:\n",
    "            if s not in vocab:\n",
    "                vocab.append_token(s)\n",
    "\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    print(f\"resume model from {model_file} weights, model args override the {model_config_file} config\")\n",
    "\n",
    "    # embedding size, number of attn heads, number of hidden dimensions, number of layers\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "    gene2idx = vocab.get_stoi()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ntokens = len(vocab)\n",
    "    print(f\"The size of the vocabulary is {ntokens}\")\n",
    "\n",
    "    # create transformer model with specified configs\n",
    "    model = TransformerModel(\n",
    "        ntokens,\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        vocab=vocab,\n",
    "        pad_value=pad_value, \n",
    "        n_input_bins=n_input_bins)\n",
    "\n",
    "    # loaded the parameters from {model_file} \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        print(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load parameters that are in the model and match the correct size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {k : v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            print(f\"Loading parameters {k} with shape {v.shape}\")\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    gene_ids = np.array([id for id in gene2idx.values()])\n",
    "    gene_embeddings = model.encoder(torch.tensor(gene_ids, dtype=torch.long).to(device))\n",
    "    gene_embeddings_vec = gene_embeddings.detach().cpu().numpy()\n",
    "    \n",
    "    assert len(gene2idx.keys()) == len(gene_embeddings)\n",
    "\n",
    "    print(\"\\nTotal number of embedding vectors:\", len(gene_embeddings))\n",
    "    print(\"Length of each vector\", len(gene_embeddings[0]))\n",
    "\n",
    "    # connecting each of the gene names with it's respective embedding array - this is the full version\n",
    "    genes_names_embeddings = dict(zip(gene2idx.keys(), gene_embeddings_vec))\n",
    "    test_key = list(genes_names_embeddings.keys())[0]\n",
    "    print(f'\\nTest Gene: {test_key}')\n",
    "    print(genes_names_embeddings[test_key])\n",
    "\n",
    "    # Save dictionary to a binary file\n",
    "    with open(output_name, 'wb') as f:\n",
    "        pickle.dump(genes_names_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5472f115-c746-47b6-92fc-efcd4f732d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scGPT_human',\n",
       " 'scGPT_heart',\n",
       " 'scGPT_bc',\n",
       " 'scGPT_kidney',\n",
       " 'scGPT_pancancer',\n",
       " 'scGPT_lung',\n",
       " 'scGPT_CP',\n",
       " 'scGPT_brain']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e8cb65d6-f04e-43dc-842d-c02c95d7960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./scgpt_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3589d09d-c4ff-4374-b3a0-208c8839c308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "scGPT_human\n",
      "resume model from scGPT_human/best_model.pt weights, model args override the scGPT_human/args.json config\n",
      "The size of the vocabulary is 60697\n",
      "Loading parameters encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "Loading parameters encoder.enc_norm.weight with shape torch.Size([512])\n",
      "Loading parameters encoder.enc_norm.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "Loading parameters value_encoder.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters value_encoder.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.weight with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.0.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "Loading parameters decoder.fc.4.bias with shape torch.Size([1])\n",
      "\n",
      "Total number of embedding vectors: 60697\n",
      "Length of each vector 512\n",
      "\n",
      "Test Gene: RP5-973N23.5\n",
      "[-1.90066600e+00  1.00820506e+00  1.70094955e+00 -1.54354262e+00\n",
      " -1.46449029e-01  4.37834620e-01  8.18119824e-01  1.52517056e+00\n",
      "  5.07942498e-01 -1.89793885e+00 -2.00963426e+00  1.66151261e+00\n",
      "  1.13980651e+00 -6.28943622e-01  1.64957359e-01  9.53273714e-01\n",
      "  2.09485292e-01  6.46140337e-01  3.80173534e-01 -9.45923746e-01\n",
      "  8.47242892e-01  1.28268600e+00  5.67622840e-01 -6.80543423e-01\n",
      "  2.13721827e-01 -9.46152806e-01  8.57212991e-02 -1.71138501e+00\n",
      "  1.40659094e+00 -3.02167445e-01 -3.65419805e-01  1.54531121e+00\n",
      "  1.68809164e+00  5.43323338e-01 -6.46542847e-01 -3.25232267e-01\n",
      " -5.37404537e-01  1.37094522e+00 -6.73127249e-02 -1.90849066e-01\n",
      "  1.23154855e+00  2.41301224e-01  2.60037482e-01 -1.63716805e+00\n",
      " -1.27275360e+00  1.37020874e+00  1.51755583e+00  8.19525421e-01\n",
      " -1.29919410e+00  6.26423657e-01  3.93228233e-01  9.50737476e-01\n",
      " -2.02602163e-01 -2.13714719e+00  5.81799209e-01 -1.60809290e+00\n",
      " -1.70450735e+00  6.24848664e-01 -1.74437618e+00 -1.45632887e+00\n",
      " -1.28191996e+00  5.78246534e-01  9.09419894e-01 -1.67715943e+00\n",
      " -1.49871612e+00  1.14716065e+00 -1.05729544e+00 -3.86793792e-01\n",
      " -1.68400085e+00 -1.21276522e+00  7.65254125e-02 -1.20931721e+00\n",
      " -4.37756985e-01  6.73336536e-02 -1.07740378e+00 -9.38634336e-01\n",
      " -1.16134524e+00  1.31133330e+00 -1.64828229e+00 -1.83225715e+00\n",
      "  5.78467548e-01  1.63245273e+00 -1.46961462e+00 -3.45052510e-01\n",
      "  5.14318943e-01 -5.61331093e-01  1.14605570e+00 -9.30610120e-01\n",
      "  1.18678784e+00  3.03114355e-01 -1.90380883e+00  1.00427363e-02\n",
      " -1.32355106e+00  1.32476962e+00 -6.68570042e-01 -5.59548259e-01\n",
      " -2.71420330e-01 -1.64587155e-01  7.40308940e-01  1.36212969e+00\n",
      "  3.53255570e-01  1.88337958e+00 -8.50054920e-01  1.80636168e+00\n",
      "  8.89338493e-01 -1.19497263e+00  2.60913461e-01  2.67723471e-01\n",
      " -8.94296587e-01 -4.46349621e-01 -1.90014529e+00  1.33383170e-01\n",
      "  2.44472906e-01  3.20331872e-01  7.23305166e-01 -1.74894288e-01\n",
      " -4.77176845e-01  1.43326297e-01 -1.69904876e+00  1.41539431e+00\n",
      " -2.35093546e+00  9.96054947e-01 -1.13652721e-01  1.00509739e+00\n",
      " -4.94227111e-01  1.53744256e+00 -2.98494138e-02 -2.05037880e+00\n",
      " -5.83446264e-01 -1.80986375e-02 -1.11871254e+00 -1.42333758e+00\n",
      "  1.49955475e+00  9.77548122e-01 -4.44840252e-01 -7.96089232e-01\n",
      " -1.68882132e+00  9.29599330e-02 -6.66731238e-01 -9.87804472e-01\n",
      " -4.98892218e-01  5.63005447e-01  1.09628797e+00  1.49495578e+00\n",
      "  1.47514176e+00 -4.01362032e-02  5.80190420e-01 -1.58731234e+00\n",
      "  4.74941194e-01  8.42816889e-01  1.43702531e+00  1.22565138e+00\n",
      " -7.19824731e-01 -1.06504238e+00  1.46840465e+00 -1.49765170e+00\n",
      "  1.47211826e+00  1.41438520e+00 -1.68943119e+00 -2.47881785e-01\n",
      " -2.14606118e+00  1.94134092e+00 -3.38945001e-01 -1.84210980e+00\n",
      "  1.56067908e+00  8.12962294e-01 -1.32411420e+00 -1.37600565e+00\n",
      " -1.14387369e+00 -1.81540155e+00  9.26779807e-01  1.72937059e+00\n",
      "  1.29652369e+00  1.59705961e+00 -1.83010793e+00  1.31083834e+00\n",
      "  5.65970302e-01 -1.32949471e+00 -1.45231009e+00  7.30963051e-01\n",
      " -6.83735251e-01  1.36828196e+00  1.48169279e+00 -4.09952372e-01\n",
      " -2.69642830e-01 -1.51445889e+00 -6.45494401e-01 -1.06960547e+00\n",
      " -6.39867246e-01  1.45029283e+00 -1.73169172e+00  6.82211280e-01\n",
      " -2.08455741e-01  1.20804422e-01 -7.58053541e-01  3.80530298e-01\n",
      "  8.85226429e-01  1.60876966e+00 -2.47823238e-01 -9.81543601e-01\n",
      "  3.95328641e-01  3.16455424e-01 -1.31239843e+00 -8.76062155e-01\n",
      " -6.06287658e-01  2.16880932e-01 -1.10901523e+00  1.40491295e+00\n",
      "  1.46135354e+00 -1.13034534e+00 -1.26308548e+00  1.50313604e+00\n",
      "  1.44035542e+00 -1.22806978e+00 -1.18412995e+00 -2.22409800e-01\n",
      " -4.32339370e-01  2.31488839e-01 -9.99165654e-01 -1.76409507e+00\n",
      " -1.83240151e+00  8.72655809e-01  3.14523280e-01  7.26763964e-01\n",
      " -1.32940948e+00 -1.00310242e+00 -1.08250833e+00 -1.79215729e+00\n",
      "  2.69508719e-01 -2.03770065e+00 -8.29541981e-01 -1.44765389e+00\n",
      "  8.19050372e-01 -1.06866646e+00  1.58332622e+00  1.18588221e+00\n",
      " -4.21002358e-01  1.89607096e+00 -1.61876905e+00 -1.13530576e+00\n",
      " -7.88215280e-01 -1.23859525e+00 -1.55071402e+00  1.07567661e-01\n",
      " -9.32501614e-01  1.96365666e+00  1.47818565e+00  2.15327665e-01\n",
      "  1.65452504e+00  1.14068198e+00  1.76992333e+00 -3.46266866e-01\n",
      " -1.30251920e+00  1.09504235e+00 -1.76190054e+00  6.73683643e-01\n",
      "  7.09246919e-02  1.02408767e-01  4.12047133e-02  1.31524086e+00\n",
      " -1.47902966e+00 -1.66045800e-01  1.06220353e+00 -4.11810309e-01\n",
      " -1.42889500e+00  6.68721855e-01 -8.27076077e-01 -5.34113288e-01\n",
      "  8.79040539e-01  1.74584925e+00 -1.46007633e+00 -4.36502099e-01\n",
      " -1.52666366e+00 -1.76928461e+00  9.65134859e-01 -1.72812298e-01\n",
      " -4.23194081e-01  5.43662727e-01  1.59059358e+00  1.32273138e+00\n",
      "  1.35907495e+00  1.03162742e+00 -7.39291728e-01  1.00432360e+00\n",
      "  4.34918076e-01  1.45482373e+00 -2.06045151e+00  9.27365839e-01\n",
      " -8.99566412e-01  1.39957774e+00  2.12870017e-01  1.34211314e+00\n",
      "  9.40270782e-01  1.46667913e-01 -3.65515321e-01  1.26457918e+00\n",
      " -1.16237211e+00  5.64205945e-01  1.06487714e-01  7.18483508e-01\n",
      "  7.10346282e-01 -5.95927298e-01  7.28287026e-02 -8.02652612e-02\n",
      "  1.48055851e+00 -4.72600937e-01  3.97271007e-01 -5.90338707e-01\n",
      "  6.71387136e-01  1.65564287e+00  3.47668141e-01 -1.12300456e+00\n",
      "  2.30998904e-01  5.81319273e-01  9.38990474e-01  5.80237687e-01\n",
      "  7.75440812e-01 -7.79715121e-01 -7.62136936e-01  1.01972759e+00\n",
      "  9.45331514e-01  1.16381145e+00  1.45491457e+00  1.71622181e+00\n",
      " -3.30456406e-01 -1.39205694e+00  9.59878266e-01  2.86938995e-01\n",
      "  1.71070278e-01 -7.23563060e-02  1.36324131e+00  5.81105709e-01\n",
      "  1.63011044e-01  3.69038373e-01  1.19775140e+00  1.12403393e+00\n",
      "  1.28758237e-01 -1.70810175e+00 -5.60754180e-01  1.58828187e+00\n",
      "  1.42231846e+00 -1.82091072e-01  8.25467825e-01  7.25536883e-01\n",
      " -1.33668816e+00 -1.75314546e-01  1.08303857e+00  7.88785875e-01\n",
      "  1.91790092e+00 -1.81728661e-01 -1.17000031e+00 -4.65701908e-01\n",
      "  1.00511479e+00  1.47848964e+00  1.33510172e+00 -1.41654646e+00\n",
      " -3.73626918e-01 -1.65250748e-01  4.37570333e-01 -1.55230439e+00\n",
      "  1.28398627e-01 -1.47119057e+00 -5.97940028e-01 -6.54916406e-01\n",
      " -1.69467080e+00 -1.07955337e+00 -1.32074487e+00 -1.12623639e-01\n",
      "  1.38707972e+00 -1.44552946e+00 -1.66253912e+00 -2.08793193e-01\n",
      " -1.68828630e+00  8.57251465e-01  2.65607554e-02 -3.74799132e-01\n",
      " -1.36463070e+00  1.68491507e+00 -1.32054484e+00 -1.06592536e+00\n",
      " -1.67954981e+00  5.78453124e-01 -1.04125631e+00  1.38479030e+00\n",
      " -1.54844058e+00 -5.13907313e-01  5.91115594e-01 -1.04316723e+00\n",
      " -6.16594076e-01  8.10311854e-01 -7.74841785e-01  1.87281299e+00\n",
      "  7.98643112e-01 -1.14572108e+00  5.12205005e-01  7.82864749e-01\n",
      " -2.31234625e-01 -9.56793249e-01 -9.98638570e-02 -1.31204808e+00\n",
      " -6.27972901e-01 -2.06896469e-01 -5.38381219e-01  9.34672296e-01\n",
      "  6.14862919e-01  1.14856160e+00  1.50065303e+00 -6.06444955e-01\n",
      "  1.55287814e+00  3.85536224e-01  1.06655562e+00  1.68380034e+00\n",
      " -9.07141984e-01  1.56956959e+00 -1.31959486e+00 -3.57569456e-01\n",
      "  1.74198568e+00 -3.16672802e-01  2.05640626e+00  9.25793648e-01\n",
      "  1.11038484e-01 -2.66030639e-01  1.01122774e-01 -9.98247027e-01\n",
      "  8.26171517e-01 -1.23427129e+00 -1.69332492e+00 -1.02744126e+00\n",
      " -1.01166964e+00 -2.92362183e-01  1.25616431e+00  1.03771113e-01\n",
      "  7.83221900e-01 -1.15620673e+00 -1.50998521e+00  7.80994773e-01\n",
      " -1.87324989e+00 -2.35554671e+00  2.08188629e+00 -9.99491096e-01\n",
      "  1.36516142e+00 -8.73981059e-01  1.11810160e+00  8.57980967e-01\n",
      "  1.26294529e+00 -2.04845810e+00 -1.98451713e-01  1.37225544e+00\n",
      "  1.46359813e+00 -1.80830002e-01 -1.95907578e-01  1.17864883e+00\n",
      "  1.36601639e+00  1.27790225e+00 -1.21346676e+00 -8.55168924e-02\n",
      "  2.86354214e-01  9.41143692e-01  2.61414945e-01 -1.22599113e+00\n",
      " -7.10933566e-01  2.64305204e-01 -3.64533544e-01 -8.21173489e-02\n",
      "  9.92912412e-01  2.02117413e-01 -1.19569778e+00  3.89738560e-01\n",
      " -1.54468334e+00  1.56951392e+00 -1.62137020e+00  1.40770233e+00\n",
      "  1.05549264e+00 -2.99786180e-02  1.44195294e+00 -4.88681674e-01\n",
      "  9.11773384e-01  3.66681457e-01  4.92369562e-01 -2.17937753e-01\n",
      "  6.56411767e-01  1.27837908e+00  3.73276174e-01 -1.90322149e+00\n",
      "  1.51590955e+00 -1.24726152e+00 -1.68594956e+00  1.73593509e+00\n",
      "  1.89565085e-02 -5.57093263e-01 -1.64988637e+00 -1.12147295e+00\n",
      "  6.35622442e-01  1.71477711e+00 -1.04629315e-01  8.34747910e-01\n",
      "  1.44085300e+00  8.05622458e-01  2.42954224e-01  1.98321685e-01\n",
      " -4.05497998e-01  1.55289273e-03 -1.51504979e-01  3.53689700e-01\n",
      " -3.67422193e-01 -5.36385477e-01 -1.29215372e+00 -1.17272317e-01\n",
      "  1.19379675e+00 -7.71149635e-01 -5.58255911e-01  4.15056139e-01]\n",
      "\n",
      "############################################################\n",
      "scGPT_heart\n",
      "resume model from scGPT_heart/best_model.pt weights, model args override the scGPT_heart/args.json config\n",
      "The size of the vocabulary is 60697\n",
      "Loading parameters encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "Loading parameters encoder.enc_norm.weight with shape torch.Size([512])\n",
      "Loading parameters encoder.enc_norm.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "Loading parameters value_encoder.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters value_encoder.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.weight with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.0.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "Loading parameters decoder.fc.4.bias with shape torch.Size([1])\n",
      "\n",
      "Total number of embedding vectors: 60697\n",
      "Length of each vector 512\n",
      "\n",
      "Test Gene: RP5-973N23.5\n",
      "[-1.7731354   1.001419    1.6759491  -1.2602571  -0.12783866  0.31972495\n",
      "  0.73717123  1.4429154   0.3890322  -1.7176833  -1.840954    1.4924991\n",
      "  1.1004939  -0.6640122  -0.03927796  0.82017976  0.03860911  0.257477\n",
      "  0.40531287 -0.64032376  0.55977     1.327709    0.5105298  -0.5151954\n",
      "  0.2717013  -1.007978    0.12532073 -1.5889406   1.5701234  -0.43271607\n",
      " -0.54142696  1.572814    1.5614702   0.5091743  -0.5616457  -0.1944606\n",
      " -0.5040364   1.0270146  -0.2561547  -0.48967955  1.2755251   0.21965082\n",
      "  0.11319458 -1.6194687  -1.270964    1.2582685   1.5046465   0.86881566\n",
      " -1.1187103   0.8910905   0.3205431   1.0748339  -0.08843433 -1.969242\n",
      "  0.5572911  -1.3425738  -1.7885804   0.6698054  -1.6391033  -1.7306887\n",
      " -0.95147794  0.6243322   0.8826046  -1.5753673  -1.2045414   0.93283737\n",
      " -0.8342322  -0.39674652 -1.5258076  -0.9348007  -0.06303482 -1.1773287\n",
      " -0.6496259   0.14970565 -1.058638   -0.7675042  -0.99839807  1.168931\n",
      " -1.3623134  -1.6307023   0.40170246  1.6565796  -1.2902853  -0.34218362\n",
      "  0.43393046 -0.38692948  1.1641153  -0.8599423   1.1519521   0.1674851\n",
      " -1.7285402   0.02779951 -1.474739    1.5749619  -0.67122906 -0.6657381\n",
      " -0.33931798 -0.20902148  0.74198544  1.2824129   0.343531    1.6874777\n",
      " -0.9549708   1.3252928   1.0138328  -1.1396573   0.24944876  0.12586997\n",
      " -0.91743064 -0.5756482  -1.8477457   0.22979216  0.40207982  0.18921167\n",
      "  0.7800463  -0.10621943 -0.4968      0.03071982 -1.4699317   1.6521058\n",
      " -1.882848    0.9703503  -0.00593841  0.998969   -0.4867223   1.4738243\n",
      "  0.14176258 -1.4804994  -0.6084574   0.02658205 -1.0893782  -1.357124\n",
      "  1.5203332   0.9881544  -0.27270985 -0.7482046  -1.490985    0.2163015\n",
      " -0.5816507  -0.7988421  -0.6658978   0.5589179   1.1239195   1.4653155\n",
      "  1.3981793   0.27547163  0.5283533  -1.5904174   0.34817478  0.81937814\n",
      "  1.2077249   1.1811001  -0.8187562  -1.4397428   1.4015163  -1.4497192\n",
      "  1.3469883   1.5899972  -1.6293919  -0.10797817 -1.7972051   1.6252887\n",
      " -0.35394198 -1.63975     1.2310728   0.7215527  -0.839016   -1.244806\n",
      " -0.8998373  -1.6278837   0.7377288   1.2544208   1.0742763   1.1580622\n",
      " -1.7707059   1.2353415   0.71475255 -1.5170976  -1.3572977   0.64503944\n",
      " -0.84077173  1.2161944   1.29532    -0.34626696 -0.1625158  -1.5237218\n",
      " -0.87143266 -0.57654834 -0.57819736  1.1844853  -1.6966535   0.65733\n",
      " -0.11382826  0.11921754 -0.8035028   0.31923503  0.8991368   1.6061723\n",
      " -0.1757496  -1.1087782   0.32357436  0.24403945 -1.2555145  -0.8981058\n",
      " -0.56360006  0.21817093 -1.2781378   0.9904825   1.5573183  -0.7341876\n",
      " -1.3299336   1.4554759   1.4935707  -1.1615429  -1.111584   -0.34577736\n",
      " -0.4035331   0.20005375 -0.95464706 -1.2714915  -1.640444    0.8103519\n",
      "  0.37888768  0.61935496 -1.4866933  -0.8456858  -0.7873673  -1.6718924\n",
      "  0.3928738  -1.6026143  -0.788415   -1.2722074   0.5965675  -1.2482257\n",
      "  1.7625207   0.88057923 -0.3393704   1.664332   -1.4282932  -1.2340106\n",
      " -0.9615992  -1.0879116  -1.631725   -0.04165323 -0.99806875  1.7938416\n",
      "  1.5840348   0.28982872  1.3275139   0.95381385  1.6373463  -0.6125439\n",
      " -1.2628601   0.98315376 -1.7576972   0.74785906  0.17470309 -0.00554559\n",
      "  0.19636479  1.3431873  -1.4795345  -0.00727743  1.1954654  -0.16148005\n",
      " -1.1668379   0.685891   -0.69490075 -0.5971409   0.97210544  1.8583409\n",
      " -1.4601079  -0.6450224  -1.4045675  -1.7099402   1.0068283  -0.07241556\n",
      " -0.5248353   0.35913193  1.4669309   1.1157713   1.3971937   0.8720559\n",
      " -0.81445646  1.0366161   0.38213575  1.5206975  -1.7843752   0.7551998\n",
      " -0.73451996  1.2448469   0.15782587  1.2602892   1.048709    0.39341396\n",
      " -0.2080939   1.0726398  -1.191986    0.6594463   0.00462099  0.76429325\n",
      "  0.71004343 -0.59292614  0.20100722  0.04801719  1.1207881  -0.4927293\n",
      "  0.41003242 -0.69540185  0.25503626  1.4759014   0.3887017  -1.2007241\n",
      "  0.26275966  0.64467657  0.9535101   0.7779383   0.8675833  -0.7594367\n",
      " -0.87963027  1.0784925   0.8933935   0.9617802   1.490469    1.5298469\n",
      " -0.4472353  -1.4642735   0.8850224   0.10360432  0.16890346  0.02724171\n",
      "  1.150657    0.40425736  0.07785915  0.21418405  1.0704001   0.92860043\n",
      "  0.16683935 -1.5762017  -0.665344    1.6411071   1.4253899  -0.01369096\n",
      "  0.8370748   0.7479441  -1.1073601  -0.26375514  1.0582707   0.689654\n",
      "  1.4667013  -0.1201469  -1.1234403  -0.52671933  0.8988893   1.463024\n",
      "  1.2682316  -1.5412186  -0.0651862  -0.2195801   0.24983275 -1.3123792\n",
      "  0.21919475 -1.1172122  -0.87845534 -0.44017658 -1.706542   -1.299512\n",
      " -1.2289509  -0.04339352  1.1424133  -1.3801162  -1.4050932  -0.09306476\n",
      " -1.6753364   0.6270926  -0.06108287 -0.40608135 -1.239451    1.5080954\n",
      " -1.2813253  -0.97716844 -1.7797916   0.34714073 -1.1344811   1.2554677\n",
      " -1.4158514  -0.5232214   0.6769158  -0.8022892  -0.4536352   0.92359173\n",
      " -0.9527073   1.5618445   0.8596062  -0.8125498   0.53482044  0.9154978\n",
      " -0.21476392 -0.91468525 -0.00522768 -1.5632538  -0.45522705 -0.13608773\n",
      " -0.32892668  1.0159642   0.7325295   1.0256119   1.2884566  -0.29888678\n",
      "  1.5476729   0.26131603  1.0144714   1.319518   -1.0348598   1.4297947\n",
      " -1.2327508  -0.5374732   1.8291284  -0.52885604  1.5538162   1.1101326\n",
      " -0.07405426 -0.30161762  0.2558792  -1.0233233   0.7748796  -1.0618662\n",
      " -1.3429781  -1.2158213  -0.9903078  -0.31287965  1.3362652   0.18462978\n",
      "  0.57385135 -1.1120551  -1.3397664   0.668907   -1.6345435  -1.5377967\n",
      "  1.6848563  -0.8581721   1.351525   -0.65451306  1.0493355   1.0727997\n",
      "  1.0396411  -1.8403028  -0.29124102  1.3131086   1.6619769  -0.18329361\n",
      " -0.03685527  1.4975419   1.5530064   1.0366749  -1.0310276  -0.08726711\n",
      "  0.29881993  0.7227878   0.36062405 -1.2219071  -0.69855696  0.2036647\n",
      " -0.4071432  -0.10116273  1.2879544   0.2948407  -1.1486068   0.33621514\n",
      " -1.2111914   1.630379   -1.4493895   1.307017    0.86929804  0.03091538\n",
      "  1.2514689  -0.36313146  0.7777622   0.3374596   0.27950227 -0.22760291\n",
      "  0.6491302   1.4609578   0.26973757 -1.7602892   1.3541352  -1.009212\n",
      " -1.5890336   1.629056   -0.05598371 -0.75799584 -1.5398396  -1.4085286\n",
      "  0.29771045  1.4800957  -0.2682751   1.2475263   1.3454584   0.47867838\n",
      "  0.25604165  0.30339125 -0.17177483 -0.00649672 -0.10253346  0.30893797\n",
      "  0.03373066 -0.42014793 -0.8700074  -0.19213937  1.040537   -0.7866117\n",
      " -0.46478662  0.5445456 ]\n",
      "\n",
      "############################################################\n",
      "scGPT_bc\n",
      "resume model from scGPT_bc/best_model.pt weights, model args override the scGPT_bc/args.json config\n",
      "The size of the vocabulary is 36574\n",
      "Loading parameters encoder.embedding.weight with shape torch.Size([36574, 512])\n",
      "Loading parameters encoder.enc_norm.weight with shape torch.Size([512])\n",
      "Loading parameters encoder.enc_norm.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "Loading parameters value_encoder.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters value_encoder.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.weight with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.0.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "Loading parameters decoder.fc.4.bias with shape torch.Size([1])\n",
      "\n",
      "Total number of embedding vectors: 36574\n",
      "Length of each vector 512\n",
      "\n",
      "Test Gene: hsa-mir-423\n",
      "[-1.0720766  -1.6595243   0.16481964  0.53802264 -1.7331971   1.1035538\n",
      "  0.78882325  0.2515329  -1.055454   -0.00982814 -0.38119254 -1.4411521\n",
      "  0.51102036 -1.7072881   0.03995998 -0.39228812  1.7824289  -0.98842007\n",
      "  2.0073097  -1.6237841  -0.85830563 -0.9848164   1.7606024  -0.24607553\n",
      "  0.9836976   0.7811537  -0.11499887 -0.98964316 -1.4598908  -0.78393626\n",
      "  1.9099872   1.7788045  -1.5616452  -0.2519637  -0.6474332  -0.30177927\n",
      "  1.5840746   1.6038415   0.55698884 -0.6512232  -0.95557714 -0.5665576\n",
      " -0.10682932  1.0671387   0.9319127   0.91438806 -0.9416724   1.3431466\n",
      " -0.21743554 -0.63732713  0.78824365 -1.0710227  -1.688541    0.6993452\n",
      " -0.12242033  0.95974064 -1.8723999   1.4179604   0.32207686  0.37308398\n",
      "  0.4624187  -1.4023366   1.2772527  -0.02991933 -1.0820918  -1.1116383\n",
      "  0.19994985  0.9738931   1.5545586  -1.5436647   0.9874229   1.4612826\n",
      "  1.4944288   1.7689896   0.5418181  -0.25150272 -1.6848608  -0.98052114\n",
      " -0.5682753   0.47921273 -1.2361776   0.85312504 -0.2763697  -1.5725065\n",
      "  1.297329    0.5821259  -1.4360083  -0.7496304   1.5221325  -0.7947049\n",
      " -1.190477    0.79413635  0.9585137   0.7239851   1.8247554   1.7548176\n",
      " -0.08494703  1.3493074  -1.3907707  -1.5153669  -1.0640044   0.23133072\n",
      " -1.8382626  -1.5610739   1.6467285   0.35114795  0.64367753  1.7935498\n",
      " -0.8075046   1.6649139  -0.8483507   1.0270187  -1.2837657  -1.1316282\n",
      "  0.982791    1.4431486  -1.0197134   0.38270354  1.3385608  -0.7197895\n",
      "  1.3272595  -1.4293405   1.2263911  -0.40981972  1.1090447   0.54007816\n",
      "  1.0066667   1.1139258  -0.00867297  1.2771376   0.8605964   0.67460793\n",
      "  1.244205   -1.7873454  -1.5075027  -0.01143531  1.2837802   0.57190156\n",
      "  0.0114752  -0.8986256  -1.2085922   1.0474868  -1.0813543  -1.7720996\n",
      "  0.71696275 -0.30279696  1.5860212  -1.004336    1.3925608  -0.90664077\n",
      " -1.0828147   1.0440251   1.2403668  -1.5376028  -0.20967361  0.51158446\n",
      " -1.3685302   0.56521255 -1.1853548  -0.17331664 -0.40535635  1.5103455\n",
      "  0.9555645   1.7721776  -0.5973827   0.8332763   0.01508739  1.4214063\n",
      "  0.3436108   1.5478175  -0.48639563 -0.45744655 -1.2461724  -1.2237988\n",
      " -0.1807085   0.36657324 -1.6946999   1.2617434  -1.5479255   0.6684215\n",
      "  0.12062318  1.470552    1.1731288   1.0340122   0.93934935 -0.05978918\n",
      "  1.7074507  -1.6078464   0.23491293  1.4773126  -0.09247822  1.3100346\n",
      "  0.03614102  0.3667288  -1.0868428   0.83201826 -1.1560317  -0.728171\n",
      " -1.3647692   1.2278636   1.5781175  -0.9711441  -0.45437843 -0.25321093\n",
      " -0.9954847  -1.3133657  -0.6488301   0.9905381  -1.6372905   0.5896542\n",
      " -0.92146033 -1.559064    0.39307678  0.7346624   1.0149752  -1.7473187\n",
      " -0.2624553  -0.48354858  0.05296711  0.9447331  -1.43719    -0.4673512\n",
      " -1.3333713   1.6819742   1.4795271  -0.32608047 -0.23594575 -0.90709597\n",
      " -0.7244035   0.31940874 -2.0260053   1.498547    1.7663095  -1.1161944\n",
      " -1.579981    0.1364189  -0.6492813  -0.5706109   0.60744804 -0.9312409\n",
      "  1.3231503   0.47719055 -1.522974   -0.29065654 -0.64813393  0.19983308\n",
      " -0.7444005   1.593062   -0.10540722  0.51613337  1.0018166  -1.2647327\n",
      " -1.0236225   1.2262877   0.48264053 -0.907687   -1.2234508  -1.4127482\n",
      " -0.41149464 -0.92090166 -1.3685931   0.828086   -1.4595635   0.02990128\n",
      "  0.28802     0.0773329  -0.15324748 -1.9259691  -0.43104306 -0.03277965\n",
      "  0.9969238  -0.56452096  1.692308    0.02828732 -0.95352596  0.27559644\n",
      "  1.1308364   0.93133134 -0.851081    0.5865861   1.7555013   1.1548036\n",
      " -0.8385277   0.9392557  -1.0644917   0.66291904  1.1646695   0.9128224\n",
      " -0.6502031   1.5811653   0.08858258 -0.26703775  0.36648542  0.87975204\n",
      " -0.543458    0.5561584  -0.80077374 -1.6050122  -1.4367492  -1.1688489\n",
      "  1.6177454   0.7883083   1.2323377  -1.5178224   1.5594164   1.6646643\n",
      "  0.997866   -0.3025554   0.45283753 -0.3223769   1.6771752  -1.6540792\n",
      " -0.28025863  0.4150597  -1.1212257  -0.89398944  0.33582085 -0.48474044\n",
      " -0.71687305 -1.3725834   0.979922    0.5978406  -0.01583161  0.9233683\n",
      "  0.7262173   1.190795    0.48365238 -1.1812987   0.3231443  -0.35864082\n",
      " -1.1302501  -1.40729    -1.1634445  -0.13597757 -1.0207713   0.20451008\n",
      "  0.622117   -0.5982022   1.5648466  -0.8907444   0.7706284  -1.5424038\n",
      "  0.35740748 -0.986271    0.6265254   1.3981367  -0.99785894  0.17836125\n",
      " -0.38570982 -1.3096592  -0.6511425   0.72856086  1.7588872   0.2294427\n",
      " -1.6014289  -0.8616052   1.8283937  -1.0684769   0.759965   -1.5210365\n",
      " -0.46840882  1.3959407  -0.18497533 -0.75804836  0.30140334  0.185965\n",
      "  1.5151128   1.4714466   0.38399217 -0.52585566  0.1868166   1.1616914\n",
      "  1.7970808  -1.6253053  -1.1431146  -1.3103122  -0.18776532  0.9589354\n",
      "  0.6660079  -1.6421472  -1.0114292   1.4943177   0.27957806 -0.8773317\n",
      " -0.7596114  -1.0452858  -0.11101215 -1.032029    1.59942     1.5780104\n",
      "  0.08492978  1.5647599   0.89103496  1.7356238  -1.4328965   0.37726933\n",
      " -0.19752988  0.22743507  1.5565096   1.5346303  -0.66802394 -0.33845803\n",
      "  1.1760532  -0.4383784  -1.1134784  -0.1378926  -1.1448884  -0.49976614\n",
      " -1.922443    1.3731987  -0.43053648 -1.6092713  -0.3365006   1.2470716\n",
      "  1.290569   -0.78464246  0.6398227  -0.2626651   0.5085057  -0.0125626\n",
      " -1.4080536  -1.4898986   0.8661679  -0.6550715   1.5464425   1.7326797\n",
      "  0.16114604  1.2655888  -0.04661943 -1.5381535  -0.59557194 -0.8312225\n",
      " -0.61884123 -0.33011818 -1.0317197   0.8153241   1.5711222   0.06477358\n",
      "  0.06093448 -1.0089499  -0.29533577 -1.269847    1.3627714  -0.8853924\n",
      " -0.32926986 -0.9355778  -1.8188002  -1.2694781  -1.3167717  -0.14909534\n",
      "  0.5383621   1.7556304   1.4077415  -1.4094992  -0.944717    1.6908197\n",
      " -0.6056431  -1.3261418   1.160968    1.1100768   1.1669286   1.8545132\n",
      " -0.60290474  1.060326    1.7875642   0.79200196 -1.0470816  -1.5614727\n",
      "  1.0036564  -0.37808636  0.42540652  1.6577095   0.45535436  1.8792518\n",
      "  0.4512601   1.3085612  -2.0337293  -1.9050168   1.2817852  -0.0950618\n",
      "  0.9589435   0.04651604  1.7358649  -1.7863401   0.1122807  -0.26745003\n",
      " -0.48850182 -1.3420663  -1.2683446   1.0537865  -1.5588536  -1.9636097\n",
      " -0.00212965 -0.2330174  -0.3352625   0.4480992   0.24593645 -0.56175077\n",
      "  1.9337949   1.2343645  -0.7665645  -1.3237004  -1.1779367  -0.518034\n",
      "  1.0036621  -0.81318754 -1.4597677   1.2424933  -0.71699506 -0.11747397\n",
      "  1.0546726  -1.4532864 ]\n",
      "\n",
      "############################################################\n",
      "scGPT_kidney\n",
      "resume model from scGPT_kidney/best_model.pt weights, model args override the scGPT_kidney/args.json config\n",
      "The size of the vocabulary is 60697\n",
      "Loading parameters encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "Loading parameters encoder.enc_norm.weight with shape torch.Size([512])\n",
      "Loading parameters encoder.enc_norm.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "Loading parameters value_encoder.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters value_encoder.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.weight with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.0.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "Loading parameters decoder.fc.4.bias with shape torch.Size([1])\n",
      "\n",
      "Total number of embedding vectors: 60697\n",
      "Length of each vector 512\n",
      "\n",
      "Test Gene: RP5-973N23.5\n",
      "[-1.7371278   0.9416683   1.6795206  -1.2811494  -0.12400466  0.28835404\n",
      "  0.6521507   1.4031155   0.36400688 -1.7917696  -1.73345     1.5232174\n",
      "  1.1524199  -0.6824792  -0.04680404  0.8414126  -0.03545773  0.23979634\n",
      "  0.4624402  -0.6668828   0.51126844  1.3906126   0.50955236 -0.48501468\n",
      "  0.31887102 -0.9565659   0.11224037 -1.5806133   1.5433736  -0.41655985\n",
      " -0.5694629   1.4941367   1.6153008   0.51275486 -0.54396945 -0.19004592\n",
      " -0.5222569   1.0604482  -0.27548668 -0.5175794   1.2379402   0.22725096\n",
      "  0.09489958 -1.7139181  -1.4037174   1.2343334   1.625007    0.89993435\n",
      " -1.0987587   0.9422054   0.3584031   1.047407   -0.09747668 -1.8742601\n",
      "  0.49045545 -1.4754983  -1.7734153   0.763747   -1.5743968  -1.7165688\n",
      " -0.9170697   0.6900084   0.80870354 -1.6634462  -1.2948159   0.8699287\n",
      " -0.8244317  -0.33053243 -1.5019058  -0.9269483  -0.11456789 -1.1706073\n",
      " -0.64494526  0.12517078 -1.0323726  -0.7475629  -0.9649086   1.1365004\n",
      " -1.3394853  -1.6017014   0.41472408  1.829522   -1.2788234  -0.34356105\n",
      "  0.43503726 -0.3729816   1.1744503  -0.78721005  1.1260821   0.21926896\n",
      " -1.6849006  -0.0084653  -1.3909962   1.5098482  -0.69286096 -0.688078\n",
      " -0.3508274  -0.17848296  0.7855336   1.3152032   0.3854031   1.664913\n",
      " -0.96524507  1.3478225   1.0287554  -1.1244386   0.28466144  0.20197086\n",
      " -0.9828824  -0.5835537  -1.7607507   0.24860568  0.38484287  0.23586234\n",
      "  0.75269055 -0.19983584 -0.5444999  -0.01124845 -1.5233297   1.6379988\n",
      " -1.7716376   0.9087146   0.00697864  1.0300875  -0.44519395  1.5397207\n",
      "  0.09740756 -1.4747825  -0.5997382   0.01173018 -1.123936   -1.3516316\n",
      "  1.5070227   1.0401497  -0.2785425  -0.6999363  -1.4603819   0.1888818\n",
      " -0.65747505 -0.798962   -0.64233625  0.64855695  1.226157    1.4573647\n",
      "  1.3731409   0.26843527  0.5530538  -1.5183458   0.3540723   0.8298504\n",
      "  1.1629125   1.2300302  -0.8269177  -1.2256303   1.3609136  -1.4781038\n",
      "  1.3435103   1.5569401  -1.5115597  -0.08735958 -1.7539347   1.6781752\n",
      " -0.3069041  -1.7491225   1.3068616   0.7272155  -0.7989598  -1.3075578\n",
      " -0.85878074 -1.7285981   0.7320608   1.1632091   1.060657    1.172694\n",
      " -1.7893409   1.2229688   0.72747743 -1.5533231  -1.4114861   0.7236575\n",
      " -0.91452736  1.2146575   1.2675496  -0.3489795  -0.15421522 -1.3500136\n",
      " -0.83295095 -0.6643974  -0.56638944  1.1708549  -1.7435993   0.6094146\n",
      " -0.13348031  0.08356658 -0.821868    0.30359724  0.8155501   1.5925218\n",
      " -0.14384776 -1.1090106   0.30339113  0.25690845 -1.2239857  -0.9149968\n",
      " -0.4803332   0.2164044  -1.2452152   0.9562147   1.5769078  -0.6618334\n",
      " -1.2560793   1.5423353   1.4646889  -1.1719327  -1.23312    -0.3632962\n",
      " -0.3790539   0.1922293  -0.8990203  -1.2795726  -1.6178317   0.80916137\n",
      "  0.39769518  0.6303427  -1.4428194  -0.82533747 -0.79615325 -1.6379772\n",
      "  0.39660978 -1.6353791  -0.7541902  -1.2586097   0.58834237 -1.3163445\n",
      "  1.6757437   0.8355225  -0.29852378  1.6286937  -1.3925616  -1.2839146\n",
      " -0.9037179  -1.078674   -1.7006646  -0.06193667 -1.012404    1.7401202\n",
      "  1.5668157   0.2855424   1.2982684   0.93273956  1.5800345  -0.59261525\n",
      " -1.2226228   1.0274394  -1.7633816   0.6980267   0.13246219 -0.01873044\n",
      "  0.18215244  1.324517   -1.3820416  -0.01528239  1.2085186  -0.18129894\n",
      " -1.0978419   0.8199173  -0.7163417  -0.5954959   1.0398896   1.7173455\n",
      " -1.511342   -0.6333316  -1.4412365  -1.8463672   1.0679091  -0.05962098\n",
      " -0.5217433   0.34410357  1.4539263   1.1717288   1.3876114   0.8818465\n",
      " -0.8829398   1.0354092   0.41326827  1.5024002  -1.7270182   0.77956736\n",
      " -0.69509834  1.254012    0.15222353  1.2470565   1.1234617   0.36820394\n",
      " -0.17670383  1.1093062  -1.1821042   0.6913603  -0.04331183  0.8543943\n",
      "  0.6544138  -0.60148364  0.20248106  0.06249665  1.0946001  -0.53614855\n",
      "  0.38360426 -0.82940966  0.2621355   1.4949876   0.40545586 -1.1907622\n",
      "  0.23800208  0.6146056   0.9886967   0.8610917   0.94001746 -0.75573444\n",
      " -0.8329281   1.1147733   0.9221735   0.9905558   1.4428656   1.3725479\n",
      " -0.45278746 -1.432284    0.8935923   0.07880685  0.19012772  0.02186759\n",
      "  1.1546633   0.3295278   0.05032228  0.2581608   1.1171545   0.94448096\n",
      "  0.22070597 -1.5967414  -0.6713041   1.6773303   1.4457934   0.00388673\n",
      "  0.86661106  0.7661926  -1.1434755  -0.25249657  0.98364455  0.6710466\n",
      "  1.4493877  -0.1884305  -1.0874165  -0.5515843   0.86277354  1.4958365\n",
      "  1.3192933  -1.6719824  -0.10226036 -0.20901679  0.2826178  -1.3326714\n",
      "  0.20321545 -1.1348103  -0.92691636 -0.40021068 -1.6999199  -1.4132942\n",
      " -1.2599205  -0.02922281  1.1501971  -1.3908123  -1.4444386  -0.08907235\n",
      " -1.6278477   0.5913005  -0.05119492 -0.42383882 -1.1890182   1.4822971\n",
      " -1.2413552  -0.998007   -1.7241347   0.32507622 -1.1131905   1.3131163\n",
      " -1.4120646  -0.5809626   0.73856795 -0.87231046 -0.46565294  1.0565264\n",
      " -0.9817718   1.4748652   0.86596954 -0.8175665   0.5133463   0.9028275\n",
      " -0.23106822 -0.9714695  -0.00679182 -1.7409809  -0.41870198 -0.15035291\n",
      " -0.3577024   0.95757806  0.70519716  1.0352666   1.3249282  -0.2735905\n",
      "  1.6330001   0.23981856  0.9532447   1.4237522  -1.0601244   1.4354742\n",
      " -1.2094709  -0.53856814  1.747678   -0.54600096  1.5063797   1.1547397\n",
      " -0.09679174 -0.28666344  0.25655907 -1.0678363   0.9829734  -1.0256817\n",
      " -1.354605   -1.2386975  -0.9630296  -0.28616554  1.5559806   0.22732121\n",
      "  0.5759859  -1.0485866  -1.388265    0.64123917 -1.6387423  -1.5019735\n",
      "  1.6416416  -0.93504655  1.3090204  -0.6135293   0.96991485  1.1734271\n",
      "  1.0969156  -1.8458258  -0.35090902  1.3049322   1.618318   -0.19383737\n",
      " -0.01757769  1.5085607   1.5826132   0.9733352  -1.0922364  -0.06108138\n",
      "  0.28326803  0.79210156  0.29657596 -1.1762042  -0.74363726  0.24578974\n",
      " -0.44338095 -0.08360779  1.4027729   0.26257214 -1.1850803   0.37506858\n",
      " -1.2171457   1.6122586  -1.4042404   1.2460821   0.83683676  0.01473354\n",
      "  1.2737983  -0.40978658  0.74326485  0.3566618   0.2189234  -0.21672422\n",
      "  0.6094245   1.5765557   0.2955645  -1.7308377   1.4126469  -1.0503211\n",
      " -1.5191644   1.5119483  -0.01454892 -0.73379093 -1.6016687  -1.5331725\n",
      "  0.26431847  1.5278175  -0.24415877  1.3033315   1.4308408   0.4114644\n",
      "  0.27293602  0.3216549  -0.13062589  0.00369468 -0.0869194   0.37901884\n",
      "  0.05310291 -0.43221346 -0.8929761  -0.19349073  1.0864906  -0.84217286\n",
      " -0.5074225   0.5244647 ]\n",
      "\n",
      "############################################################\n",
      "scGPT_pancancer\n",
      "resume model from scGPT_pancancer/best_model.pt weights, model args override the scGPT_pancancer/args.json config\n",
      "The size of the vocabulary is 60697\n",
      "Loading parameters encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "Loading parameters encoder.enc_norm.weight with shape torch.Size([512])\n",
      "Loading parameters encoder.enc_norm.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "Loading parameters value_encoder.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters value_encoder.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.weight with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.0.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "Loading parameters decoder.fc.4.bias with shape torch.Size([1])\n",
      "\n",
      "Total number of embedding vectors: 60697\n",
      "Length of each vector 512\n",
      "\n",
      "Test Gene: RP5-973N23.5\n",
      "[-1.61047602e+00  9.67100561e-01  1.74378884e+00 -1.27163267e+00\n",
      " -3.82023305e-01  5.52934229e-01  6.68334246e-01  1.17251015e+00\n",
      "  5.21529734e-01 -1.81782031e+00 -1.65813935e+00  1.52276731e+00\n",
      "  1.06934965e+00 -3.73118281e-01  9.46308672e-02  9.42281723e-01\n",
      "  3.01595122e-01  5.39101958e-01  3.07756573e-01 -5.98243058e-01\n",
      "  7.21431136e-01  1.22508085e+00  7.14007497e-01 -7.06230462e-01\n",
      "  3.02747518e-01 -8.99002910e-01  7.99968094e-02 -1.60828090e+00\n",
      "  1.34925282e+00 -2.08097100e-01 -3.64653081e-01  1.55982721e+00\n",
      "  1.57947183e+00  7.24276662e-01 -1.05453640e-01 -3.14384878e-01\n",
      " -3.94016117e-01  1.09560943e+00  4.60902192e-02 -3.05962622e-01\n",
      "  1.31131399e+00  2.42980227e-01  7.62169436e-02 -1.38360488e+00\n",
      " -8.75261486e-01  1.38013518e+00  1.45857823e+00  1.10957932e+00\n",
      " -1.33693945e+00  4.52664614e-01  3.71689141e-01  1.05201423e+00\n",
      " -2.66642034e-01 -1.81274688e+00  7.01250792e-01 -1.60729015e+00\n",
      " -1.34246433e+00  6.36181235e-01 -1.57070601e+00 -1.14885688e+00\n",
      " -1.09756160e+00  8.41917157e-01  6.99513018e-01 -1.65997720e+00\n",
      " -1.12534702e+00  1.20336998e+00 -9.57305551e-01 -3.77782464e-01\n",
      " -1.60607040e+00 -1.09239233e+00  1.57996312e-01 -9.39064503e-01\n",
      " -6.91622913e-01  9.14225727e-02 -1.06827676e+00 -1.03084624e+00\n",
      " -1.13630283e+00  9.84444439e-01 -1.71075535e+00 -1.55276823e+00\n",
      "  2.57116228e-01  1.78437924e+00 -1.48297536e+00 -3.99141163e-01\n",
      "  3.52422982e-01 -4.89591926e-01  1.30443871e+00 -9.65951741e-01\n",
      "  1.18216813e+00  4.89630282e-01 -1.72643983e+00  4.19583470e-02\n",
      " -1.43992984e+00  1.40953469e+00 -7.46375859e-01 -5.62468827e-01\n",
      " -8.06479976e-02 -1.82449400e-01  7.25128472e-01  1.39317489e+00\n",
      "  4.46050107e-01  1.96141648e+00 -9.50956762e-01  1.87059355e+00\n",
      "  6.87483907e-01 -1.02016830e+00  3.26700598e-01 -1.51552156e-01\n",
      " -8.62819433e-01 -4.48803723e-01 -1.78148484e+00 -2.99948063e-02\n",
      "  5.55107951e-01  3.31967771e-01  7.85145044e-01 -1.23782650e-01\n",
      " -3.76337647e-01  2.39521295e-01 -1.66998410e+00  1.08807659e+00\n",
      " -1.87314045e+00  7.71600425e-01 -8.52335542e-02  1.11590159e+00\n",
      " -5.76488376e-01  1.45767784e+00  2.61958420e-01 -1.68099725e+00\n",
      " -5.31875491e-01 -7.59050548e-02 -1.07227969e+00 -1.42728078e+00\n",
      "  1.27548420e+00  7.37169266e-01 -3.91021132e-01 -7.95370460e-01\n",
      " -1.65873837e+00 -7.84784555e-04 -6.34441853e-01 -1.17384005e+00\n",
      " -7.15605795e-01  4.93090332e-01  1.13646042e+00  1.49171567e+00\n",
      "  1.51080608e+00 -1.18243182e-02  4.11547840e-01 -1.35593641e+00\n",
      "  1.54407710e-01  9.69205439e-01  1.34645617e+00  1.27453208e+00\n",
      " -6.86154246e-01 -1.19163203e+00  1.37426817e+00 -1.39375269e+00\n",
      "  1.17916298e+00  1.47167730e+00 -1.21936703e+00 -1.34672418e-01\n",
      " -1.81917262e+00  1.65268016e+00 -4.27500635e-01 -1.51677334e+00\n",
      "  1.60534263e+00  6.88601971e-01 -1.01911569e+00 -1.27347112e+00\n",
      " -1.04870474e+00 -1.78090310e+00  8.80331635e-01  1.45157039e+00\n",
      "  1.15504313e+00  1.49750376e+00 -1.70042908e+00  1.33831954e+00\n",
      "  3.13562691e-01 -9.99464750e-01 -1.37588096e+00  6.19960189e-01\n",
      " -5.72308600e-01  1.35768402e+00  1.29775155e+00 -7.43288472e-02\n",
      " -1.97534889e-01 -1.41839075e+00 -6.96370542e-01 -7.62147725e-01\n",
      " -7.64790893e-01  1.24394476e+00 -1.60883844e+00  6.24602556e-01\n",
      " -2.50295818e-01  2.04006910e-01 -9.04819310e-01  4.48281139e-01\n",
      "  6.83503807e-01  1.52398181e+00 -3.58915418e-01 -1.05825698e+00\n",
      "  1.74924552e-01  1.36982456e-01 -1.43337631e+00 -9.52328861e-01\n",
      " -5.06639957e-01  1.21954098e-01 -1.19550610e+00  1.20925069e+00\n",
      "  1.22413087e+00 -1.03796291e+00 -1.02431130e+00  1.61733067e+00\n",
      "  1.45846903e+00 -1.00323153e+00 -1.06875193e+00 -2.68421769e-01\n",
      " -4.43139374e-01  9.50661153e-02 -9.17663991e-01 -1.17090392e+00\n",
      " -1.87464941e+00  8.05319726e-01  4.90662903e-02  8.26572895e-01\n",
      " -1.39620221e+00 -1.06856763e+00 -9.37724054e-01 -1.99548590e+00\n",
      "  2.84397691e-01 -1.91168809e+00 -7.85573065e-01 -1.14953136e+00\n",
      "  9.95790184e-01 -8.70442092e-01  1.63458061e+00  1.09172451e+00\n",
      " -4.09834206e-01  1.85049891e+00 -1.54934943e+00 -7.73016870e-01\n",
      " -8.20636749e-01 -9.78953719e-01 -1.76642239e+00  1.31997079e-01\n",
      " -5.03498912e-01  1.90813458e+00  1.31077099e+00  4.46322322e-01\n",
      "  1.45064652e+00  1.00049675e+00  1.51358330e+00 -2.18691424e-01\n",
      " -1.10291302e+00  1.03107095e+00 -1.78747010e+00  7.18963802e-01\n",
      "  6.05559461e-02  1.61471099e-01  8.60720277e-02  1.32295656e+00\n",
      " -1.38795209e+00  4.17002710e-03  1.15861225e+00 -3.99906784e-01\n",
      " -1.18185985e+00  6.71539307e-01 -6.47554696e-01 -5.90896666e-01\n",
      "  8.10036123e-01  1.78788900e+00 -1.26377475e+00 -4.47777927e-01\n",
      " -1.50114942e+00 -1.60648489e+00  8.02483618e-01  1.16990484e-01\n",
      " -3.65852684e-01  5.11004388e-01  1.69821358e+00  1.15724838e+00\n",
      "  1.41291094e+00  6.23577714e-01 -6.90773964e-01  8.73248458e-01\n",
      "  4.44003254e-01  1.48124552e+00 -1.81940782e+00  7.44037390e-01\n",
      " -9.37724829e-01  1.43293226e+00  2.29672417e-01  1.33208609e+00\n",
      "  1.11226356e+00  1.59356877e-01 -2.67962962e-01  1.06491387e+00\n",
      " -1.13194036e+00  8.10417175e-01  2.66948253e-01  9.46243823e-01\n",
      "  9.90057349e-01 -7.03679264e-01  2.27019757e-01  6.53570220e-02\n",
      "  1.40430474e+00 -3.15241694e-01  4.67315823e-01 -4.09448385e-01\n",
      "  6.38250053e-01  1.53312755e+00  4.15107250e-01 -7.19798565e-01\n",
      "  2.18465999e-01  5.35772026e-01  8.55315268e-01  4.29209799e-01\n",
      "  9.23988521e-01 -5.91404259e-01 -6.15958929e-01  1.00519860e+00\n",
      "  6.59452796e-01  1.26487887e+00  1.73024094e+00  1.60767817e+00\n",
      " -4.04711962e-01 -1.32724261e+00  7.51075149e-01  2.87651390e-01\n",
      "  1.51448950e-01 -7.38501474e-02  9.92671132e-01  6.39643490e-01\n",
      " -3.88854519e-02  3.31291348e-01  1.09250998e+00  9.43595350e-01\n",
      " -9.48961005e-02 -1.62005663e+00 -4.15462852e-01  1.34552336e+00\n",
      "  1.16770196e+00 -1.28832445e-01  6.29229009e-01  7.48507857e-01\n",
      " -9.16101158e-01 -2.45032534e-01  9.66658950e-01  8.24763477e-01\n",
      "  1.67320085e+00 -2.21609160e-01 -1.29909122e+00 -3.27969849e-01\n",
      "  8.43834817e-01  1.31385672e+00  1.29933178e+00 -1.50618339e+00\n",
      " -7.74657279e-02 -1.63834840e-01  1.54765904e-01 -1.33169305e+00\n",
      "  1.51535302e-01 -1.29842079e+00 -5.47473788e-01 -5.71040928e-01\n",
      " -1.61197388e+00 -5.99994004e-01 -1.39736986e+00 -1.55206189e-01\n",
      "  1.64343786e+00 -1.36129797e+00 -1.45250595e+00 -9.91912112e-02\n",
      " -1.88193011e+00  8.51260900e-01  2.48021513e-01 -3.31545204e-01\n",
      " -1.37841225e+00  1.68166637e+00 -1.06696868e+00 -9.77493584e-01\n",
      " -1.39584851e+00  5.86521685e-01 -1.24928808e+00  1.30467737e+00\n",
      " -1.51433063e+00 -3.05596799e-01  4.49718803e-01 -1.09335947e+00\n",
      " -7.31094778e-01  8.58830869e-01 -7.35233724e-01  1.87729537e+00\n",
      "  5.76270401e-01 -1.11313939e+00  4.18360651e-01  7.20595598e-01\n",
      " -3.45040530e-01 -7.39421427e-01 -1.29461110e-01 -1.24903047e+00\n",
      " -5.18250346e-01  1.40866144e-02 -2.79567629e-01  8.88578355e-01\n",
      "  6.71908081e-01  7.17968583e-01  1.49034762e+00 -4.65803355e-01\n",
      "  1.70175755e+00  4.36763316e-01  9.54258859e-01  1.40577376e+00\n",
      " -9.19506073e-01  1.75336015e+00 -1.13880491e+00 -5.58302402e-01\n",
      "  1.84111965e+00 -4.34389889e-01  1.57452476e+00  1.30125153e+00\n",
      "  2.59408772e-01 -3.78018916e-01  3.24730605e-01 -6.70194089e-01\n",
      "  7.05312431e-01 -1.10259807e+00 -1.19963872e+00 -1.23866010e+00\n",
      " -1.00329673e+00 -2.74129719e-01  1.12233996e+00 -5.33327684e-02\n",
      "  7.96507657e-01 -1.19238770e+00 -1.47370327e+00  8.97588849e-01\n",
      " -1.69850791e+00 -2.05592608e+00  2.22831655e+00 -7.35080481e-01\n",
      "  1.56335950e+00 -8.43228996e-01  1.10251951e+00  6.63945258e-01\n",
      "  1.05731344e+00 -1.82908928e+00 -7.52429366e-02  1.39113104e+00\n",
      "  1.02108669e+00 -1.73821613e-01 -2.93709725e-01  1.24209321e+00\n",
      "  1.45976317e+00  1.21193421e+00 -1.11548519e+00 -1.57345474e-01\n",
      "  3.27177823e-01  7.69383550e-01  2.72770822e-01 -1.31115448e+00\n",
      " -9.09923613e-01  3.43247682e-01 -1.42957985e-01 -1.20795548e-01\n",
      "  1.12469363e+00  3.51473272e-01 -9.74891901e-01  2.63934731e-01\n",
      " -1.38347352e+00  1.34132957e+00 -1.67310143e+00  1.41815495e+00\n",
      "  1.03107035e+00  2.10652780e-02  1.36895227e+00 -4.57759172e-01\n",
      "  7.50845313e-01  4.16450143e-01  5.06645501e-01 -1.22254059e-01\n",
      "  6.57067895e-01  1.03618443e+00  3.67142946e-01 -1.78097320e+00\n",
      "  9.71920133e-01 -1.08414531e+00 -1.62922919e+00  1.68322814e+00\n",
      "  5.54509759e-02 -5.11492729e-01 -1.62000263e+00 -1.04468322e+00\n",
      "  6.90751076e-01  1.50217700e+00 -9.33730006e-02  1.03709126e+00\n",
      "  1.50423133e+00  8.20990264e-01  2.48600572e-01  2.78264016e-01\n",
      " -3.87622982e-01 -6.88359812e-02 -1.40457973e-01  4.74949062e-01\n",
      " -6.49129376e-02 -4.23003346e-01 -1.04771841e+00 -2.60186821e-01\n",
      "  1.24249375e+00 -6.09636128e-01 -3.64769459e-01  5.62037766e-01]\n",
      "\n",
      "############################################################\n",
      "scGPT_lung\n",
      "resume model from scGPT_lung/best_model.pt weights, model args override the scGPT_lung/args.json config\n",
      "The size of the vocabulary is 60697\n",
      "Loading parameters encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "Loading parameters encoder.enc_norm.weight with shape torch.Size([512])\n",
      "Loading parameters encoder.enc_norm.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "Loading parameters value_encoder.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters value_encoder.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.weight with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.0.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "Loading parameters decoder.fc.4.bias with shape torch.Size([1])\n",
      "\n",
      "Total number of embedding vectors: 60697\n",
      "Length of each vector 512\n",
      "\n",
      "Test Gene: RP5-973N23.5\n",
      "[-1.6809043   1.0210818   1.6536825  -1.3256707  -0.11974102  0.30987144\n",
      "  0.70217353  1.4139191   0.4119791  -1.8779438  -1.7819617   1.4853806\n",
      "  1.1595203  -0.6708855  -0.00982208  0.8708784   0.02840355  0.2537991\n",
      "  0.4182681  -0.70354694  0.5384773   1.3424845   0.50908256 -0.5125361\n",
      "  0.32777625 -0.9907817   0.12458629 -1.5508716   1.5113646  -0.38351628\n",
      " -0.5689548   1.4726769   1.7000003   0.52229244 -0.5242575  -0.23261023\n",
      " -0.55816     1.0884211  -0.22941135 -0.42863098  1.2408363   0.23210393\n",
      "  0.14253037 -1.7464136  -1.3712413   1.2164985   1.7463667   0.9478391\n",
      " -1.1069463   0.86490554  0.35886052  1.0303155  -0.1269848  -2.0745769\n",
      "  0.49873427 -1.5012996  -1.8023745   0.76008874 -1.5423579  -1.6322118\n",
      " -0.98870194  0.6526371   0.8694631  -1.7281228  -1.2949617   0.9290153\n",
      " -0.8192412  -0.35746822 -1.644832   -0.95015216 -0.12362908 -1.1328394\n",
      " -0.6688155   0.12698711 -1.0770488  -0.897663   -1.009524    1.1340278\n",
      " -1.4626409  -1.6893814   0.420184    1.6997938  -1.4316168  -0.36908486\n",
      "  0.45093098 -0.3945502   1.2007877  -0.776798    1.1261479   0.2122781\n",
      " -1.8160068  -0.04225517 -1.4625105   1.584781   -0.6914282  -0.6855009\n",
      " -0.3167958  -0.13643639  0.8694082   1.2561066   0.37204313  1.775804\n",
      " -0.9841803   1.3845718   1.01889    -1.1828401   0.29517746  0.11395153\n",
      " -0.9169152  -0.56961393 -1.8709016   0.2390514   0.36947092  0.19396491\n",
      "  0.8405627  -0.22319594 -0.5136267   0.03508226 -1.4910758   1.5949706\n",
      " -1.8767108   0.90697193  0.00372131  1.1759413  -0.43936107  1.5546491\n",
      "  0.11500103 -1.569043   -0.6133799   0.01991075 -1.1095487  -1.3574263\n",
      "  1.5987984   1.0599178  -0.29323584 -0.7579813  -1.5716815   0.19933975\n",
      " -0.6566635  -0.8500781  -0.64354026  0.6394335   1.1834193   1.5359491\n",
      "  1.3881527   0.22149646  0.56392777 -1.5904421   0.32360733  0.818615\n",
      "  1.2209959   1.23387    -0.8164291  -1.2438214   1.3702978  -1.3768873\n",
      "  1.3959178   1.5778884  -1.4924101  -0.11124028 -1.7974135   1.6647705\n",
      " -0.31495276 -1.6404876   1.4250067   0.7477168  -0.94514537 -1.3763928\n",
      " -0.8546478  -1.7631347   0.80709904  1.3779789   1.1031183   1.173699\n",
      " -1.7961602   1.2600071   0.70284474 -1.546262   -1.4631124   0.6701299\n",
      " -0.8460382   1.2084974   1.3372774  -0.34886885 -0.20497848 -1.4402391\n",
      " -0.82902414 -0.65116805 -0.61795205  1.2932708  -1.8471678   0.6734299\n",
      " -0.1320582   0.14148758 -0.8061473   0.33014706  0.82841164  1.5733908\n",
      " -0.18479796 -1.0703071   0.3241879   0.2820326  -1.4533494  -0.9192165\n",
      " -0.48367724  0.23189196 -1.2816375   0.9559419   1.6245002  -0.79701906\n",
      " -1.2895129   1.5777819   1.4756356  -1.1912744  -1.2343872  -0.36718336\n",
      " -0.44079348  0.21262851 -0.9223037  -1.4503325  -1.713831    0.8040373\n",
      "  0.36634037  0.5906422  -1.4329615  -0.8735259  -0.82570904 -1.6761076\n",
      "  0.47064734 -1.726318   -0.78814274 -1.4072762   0.60986245 -1.2443058\n",
      "  1.87232     0.9187293  -0.34195694  1.6538476  -1.4198976  -1.226548\n",
      " -0.9333772  -1.0813456  -1.6663961  -0.03588247 -1.0154088   1.7543579\n",
      "  1.7416873   0.33051783  1.3590764   0.9840569   1.582634   -0.53370315\n",
      " -1.2358245   1.0557529  -1.7281373   0.7069778   0.14133807  0.01293264\n",
      "  0.18211186  1.3231914  -1.3738627  -0.04824467  1.270188   -0.24335155\n",
      " -1.1027628   0.83273876 -0.6778521  -0.51381993  0.9674297   1.7557328\n",
      " -1.4517082  -0.62611693 -1.4695752  -1.7136877   1.0480931  -0.0224618\n",
      " -0.48710233  0.40456188  1.422321    1.1906024   1.5385377   0.87296724\n",
      " -0.8957408   1.0336214   0.40207902  1.5290419  -1.779971    0.840952\n",
      " -0.7456725   1.3969448   0.1724601   1.2617894   1.1193967   0.37422574\n",
      " -0.17500588  1.1068455  -1.2867708   0.6842164  -0.01488675  0.95479095\n",
      "  0.7353319  -0.583746    0.1948138   0.00944974  1.1424547  -0.5328974\n",
      "  0.40397185 -0.8057667   0.3028686   1.5681508   0.332075   -1.1897072\n",
      "  0.22935157  0.63900256  0.9481243   0.75595677  0.947852   -0.6952341\n",
      " -0.8704234   1.0421427   0.8507663   1.0228093   1.4848866   1.4038029\n",
      " -0.47275802 -1.4238391   0.90877396  0.10560204  0.18055156  0.04849281\n",
      "  1.1308721   0.4061983   0.05918397  0.25390422  1.1049002   1.0444711\n",
      "  0.2068232  -1.5669703  -0.65685046  1.6619531   1.4224418  -0.04517664\n",
      "  0.8126884   0.74901927 -1.157072   -0.26207653  1.0747956   0.6589367\n",
      "  1.5372218  -0.17696033 -1.1299493  -0.587623    0.90082663  1.4687732\n",
      "  1.308464   -1.6537653  -0.06829377 -0.20635985  0.28472805 -1.3242568\n",
      "  0.2142164  -1.221905   -0.90882224 -0.43141583 -1.6759076  -1.3106749\n",
      " -1.2397697  -0.03069207  1.3012872  -1.4161899  -1.3727438  -0.09137118\n",
      " -1.7595658   0.6464908  -0.02534725 -0.44116843 -1.2529742   1.5652481\n",
      " -1.2348113  -1.0292504  -1.7666483   0.3672352  -1.1136738   1.3169346\n",
      " -1.4560843  -0.53499323  0.7123326  -0.9266571  -0.4543303   0.9860338\n",
      " -1.0242606   1.4765497   0.86825895 -0.86987686  0.5281091   0.9369703\n",
      " -0.2209279  -1.0349573  -0.04537703 -1.7856046  -0.44505727 -0.15589854\n",
      " -0.37624547  0.94107866  0.78040993  1.07032     1.336524   -0.30948406\n",
      "  1.5884913   0.304843    1.0625291   1.5402632  -1.0633874   1.5064496\n",
      " -1.223157   -0.52747285  1.8222823  -0.5257205   1.5608181   1.0821388\n",
      " -0.084176   -0.2892756   0.26480678 -1.036039    0.971163   -1.0742259\n",
      " -1.3760773  -1.1735957  -0.91477317 -0.2886395   1.412391    0.20164596\n",
      "  0.57753164 -1.0863154  -1.3479683   0.7184954  -1.6657578  -1.6707014\n",
      "  1.6458037  -0.9057435   1.4196849  -0.6819781   0.94517195  1.0614072\n",
      "  1.1189867  -1.8177651  -0.3262349   1.3269645   1.6591356  -0.23091038\n",
      " -0.03113279  1.3817916   1.6117159   1.1749505  -1.1558948  -0.05617192\n",
      "  0.34697464  0.7644904   0.25743508 -1.2065254  -0.7787236   0.2808715\n",
      " -0.43257046 -0.09282026  1.3370063   0.27464104 -1.2606701   0.3582921\n",
      " -1.2717369   1.5527383  -1.5353916   1.3140473   0.8753972  -0.00688954\n",
      "  1.2803172  -0.3982406   0.7989468   0.38537312  0.26573    -0.21339567\n",
      "  0.5958496   1.4128945   0.31410927 -1.6822919   1.4435716  -1.098117\n",
      " -1.5859919   1.5892024  -0.02353902 -0.75127167 -1.6737654  -1.4207902\n",
      "  0.3644711   1.552373   -0.18133633  1.2790467   1.4391371   0.5177923\n",
      "  0.25206175  0.33323827 -0.14535482  0.01068109 -0.12730795  0.41019833\n",
      "  0.05416674 -0.4261128  -0.9559586  -0.19678694  1.1153265  -0.831635\n",
      " -0.4534854   0.57215154]\n",
      "\n",
      "############################################################\n",
      "scGPT_CP\n",
      "resume model from scGPT_CP/best_model.pt weights, model args override the scGPT_CP/args.json config\n",
      "The size of the vocabulary is 60697\n",
      "Loading parameters encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "Loading parameters encoder.enc_norm.weight with shape torch.Size([512])\n",
      "Loading parameters encoder.enc_norm.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "Loading parameters value_encoder.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters value_encoder.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.weight with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.0.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "Loading parameters decoder.fc.4.bias with shape torch.Size([1])\n",
      "Loading parameters cls_decoder._decoder.0.weight with shape torch.Size([512, 512])\n",
      "Loading parameters cls_decoder._decoder.0.bias with shape torch.Size([512])\n",
      "Loading parameters cls_decoder._decoder.2.weight with shape torch.Size([512])\n",
      "Loading parameters cls_decoder._decoder.2.bias with shape torch.Size([512])\n",
      "Loading parameters cls_decoder._decoder.3.weight with shape torch.Size([512, 512])\n",
      "Loading parameters cls_decoder._decoder.3.bias with shape torch.Size([512])\n",
      "Loading parameters cls_decoder._decoder.5.weight with shape torch.Size([512])\n",
      "Loading parameters cls_decoder._decoder.5.bias with shape torch.Size([512])\n",
      "\n",
      "Total number of embedding vectors: 60697\n",
      "Length of each vector 512\n",
      "\n",
      "Test Gene: RP5-973N23.5\n",
      "[-1.91945624e+00  1.00058711e+00  1.70368922e+00 -1.54656887e+00\n",
      " -1.47874922e-01  4.39718962e-01  8.26662838e-01  1.52678597e+00\n",
      "  5.11961937e-01 -1.91736341e+00 -2.01606178e+00  1.66079569e+00\n",
      "  1.13294828e+00 -6.31070077e-01  1.62429824e-01  9.69381630e-01\n",
      "  2.12089777e-01  6.44003928e-01  3.74453366e-01 -9.49893475e-01\n",
      "  8.47554564e-01  1.27392864e+00  5.70161760e-01 -6.81307793e-01\n",
      "  2.17261240e-01 -9.54852402e-01  8.40235874e-02 -1.70550239e+00\n",
      "  1.41064608e+00 -3.03121835e-01 -3.63158435e-01  1.55800819e+00\n",
      "  1.70815241e+00  5.46852350e-01 -6.53192997e-01 -3.22229266e-01\n",
      " -5.37944674e-01  1.38523662e+00 -6.80323616e-02 -1.91671193e-01\n",
      "  1.22646224e+00  2.41333440e-01  2.52871096e-01 -1.64707494e+00\n",
      " -1.26567519e+00  1.37061596e+00  1.52401173e+00  8.20062339e-01\n",
      " -1.28801644e+00  6.25001252e-01  3.90434176e-01  9.42881942e-01\n",
      " -1.99020669e-01 -2.15274787e+00  5.87766588e-01 -1.62293994e+00\n",
      " -1.70304871e+00  6.26463830e-01 -1.74365163e+00 -1.45710850e+00\n",
      " -1.28731525e+00  5.83271086e-01  9.10029113e-01 -1.67566192e+00\n",
      " -1.51110137e+00  1.13097036e+00 -1.05610704e+00 -3.79888624e-01\n",
      " -1.70057940e+00 -1.21993196e+00  7.89000466e-02 -1.20787787e+00\n",
      " -4.42262203e-01  7.27313608e-02 -1.07665849e+00 -9.41555142e-01\n",
      " -1.16445851e+00  1.31474781e+00 -1.64488280e+00 -1.83723617e+00\n",
      "  5.82665145e-01  1.62907243e+00 -1.46422029e+00 -3.39401782e-01\n",
      "  5.11517048e-01 -5.59221506e-01  1.14388406e+00 -9.32186663e-01\n",
      "  1.18627155e+00  3.01385969e-01 -1.91326869e+00  7.26746954e-03\n",
      " -1.32950366e+00  1.31708586e+00 -6.66397810e-01 -5.64123392e-01\n",
      " -2.78454900e-01 -1.65187746e-01  7.36163855e-01  1.36493552e+00\n",
      "  3.54273468e-01  1.88447630e+00 -8.53097975e-01  1.79684389e+00\n",
      "  8.77922952e-01 -1.20444191e+00  2.69473106e-01  2.70675480e-01\n",
      " -9.01304126e-01 -4.48697507e-01 -1.89814401e+00  1.32234991e-01\n",
      "  2.42665365e-01  3.18752438e-01  7.20064878e-01 -1.73882365e-01\n",
      " -4.82425630e-01  1.43959418e-01 -1.70305383e+00  1.40934181e+00\n",
      " -2.37771678e+00  1.00113750e+00 -1.12625696e-01  1.01228070e+00\n",
      " -4.93792683e-01  1.55288577e+00 -2.28037853e-02 -2.05504942e+00\n",
      " -5.84123611e-01 -1.67255029e-02 -1.13014126e+00 -1.42867196e+00\n",
      "  1.50553465e+00  9.89309192e-01 -4.48786587e-01 -8.01189661e-01\n",
      " -1.69851577e+00  9.24271420e-02 -6.65031672e-01 -9.85329747e-01\n",
      " -5.02420485e-01  5.70134401e-01  1.10360932e+00  1.50482512e+00\n",
      "  1.48076117e+00 -4.68426161e-02  5.85387647e-01 -1.58686304e+00\n",
      "  4.75756317e-01  8.42248499e-01  1.45475030e+00  1.21097469e+00\n",
      " -7.14193821e-01 -1.06799603e+00  1.47194302e+00 -1.49810004e+00\n",
      "  1.46928596e+00  1.40571320e+00 -1.69533372e+00 -2.50034243e-01\n",
      " -2.16095352e+00  1.93678606e+00 -3.40303063e-01 -1.85490167e+00\n",
      "  1.56340170e+00  8.11624169e-01 -1.32988465e+00 -1.37216103e+00\n",
      " -1.14852357e+00 -1.83208787e+00  9.33181345e-01  1.73328769e+00\n",
      "  1.30489278e+00  1.59129000e+00 -1.84972644e+00  1.30106270e+00\n",
      "  5.64188778e-01 -1.32551777e+00 -1.45859337e+00  7.33219147e-01\n",
      " -6.75454438e-01  1.36310983e+00  1.48416960e+00 -4.05678362e-01\n",
      " -2.70421922e-01 -1.53670824e+00 -6.45945966e-01 -1.06925118e+00\n",
      " -6.33128583e-01  1.45533419e+00 -1.73652112e+00  6.83363914e-01\n",
      " -2.08828643e-01  1.21063113e-01 -7.59345949e-01  3.86759967e-01\n",
      "  8.90991092e-01  1.61341262e+00 -2.60303855e-01 -9.82263446e-01\n",
      "  3.97294760e-01  3.17281663e-01 -1.31762540e+00 -8.70453656e-01\n",
      " -6.05642855e-01  2.15979844e-01 -1.11078358e+00  1.40286624e+00\n",
      "  1.47068310e+00 -1.13120937e+00 -1.28211129e+00  1.50565434e+00\n",
      "  1.45359468e+00 -1.22979271e+00 -1.19349086e+00 -2.22169876e-01\n",
      " -4.40147817e-01  2.34297216e-01 -1.00023341e+00 -1.77826726e+00\n",
      " -1.83461714e+00  8.80891979e-01  3.09930414e-01  7.29215741e-01\n",
      " -1.33667088e+00 -9.99877810e-01 -1.07700801e+00 -1.78586423e+00\n",
      "  2.65314370e-01 -2.04960823e+00 -8.23738754e-01 -1.44491708e+00\n",
      "  8.17012072e-01 -1.05645287e+00  1.57343113e+00  1.18935525e+00\n",
      " -4.15427923e-01  1.89807749e+00 -1.62772155e+00 -1.14060414e+00\n",
      " -7.86161125e-01 -1.24572980e+00 -1.55614781e+00  1.00646846e-01\n",
      " -9.37861741e-01  1.97811282e+00  1.49179220e+00  2.17590258e-01\n",
      "  1.65995944e+00  1.14716196e+00  1.79429507e+00 -3.41397583e-01\n",
      " -1.31498730e+00  1.09291899e+00 -1.75482261e+00  6.76508904e-01\n",
      "  7.63034672e-02  1.04322374e-01  3.98878939e-02  1.31459916e+00\n",
      " -1.47225749e+00 -1.70466110e-01  1.05511248e+00 -4.07346338e-01\n",
      " -1.43402612e+00  6.78944290e-01 -8.26248884e-01 -5.29713750e-01\n",
      "  8.82635176e-01  1.73951602e+00 -1.46194756e+00 -4.36278105e-01\n",
      " -1.53621423e+00 -1.77161002e+00  9.66875196e-01 -1.72803789e-01\n",
      " -4.25860137e-01  5.44225991e-01  1.59193623e+00  1.32327139e+00\n",
      "  1.35516047e+00  1.02251256e+00 -7.36486554e-01  1.00524950e+00\n",
      "  4.36606944e-01  1.44761658e+00 -2.06811380e+00  9.26247239e-01\n",
      " -9.07644510e-01  1.41264892e+00  2.15965420e-01  1.34512532e+00\n",
      "  9.44151044e-01  1.49605900e-01 -3.68263721e-01  1.27218235e+00\n",
      " -1.17225027e+00  5.65553486e-01  1.03040546e-01  7.13482380e-01\n",
      "  6.94935799e-01 -5.99928558e-01  7.72468373e-02 -7.96215236e-02\n",
      "  1.48344743e+00 -4.72873867e-01  4.01061416e-01 -5.88429213e-01\n",
      "  6.66000783e-01  1.65624332e+00  3.48432004e-01 -1.12209737e+00\n",
      "  2.35827729e-01  5.69713056e-01  9.47920203e-01  5.81284523e-01\n",
      "  7.81325042e-01 -7.82648265e-01 -7.66961694e-01  1.02719808e+00\n",
      "  9.43963289e-01  1.16536963e+00  1.46672583e+00  1.72122562e+00\n",
      " -3.31228673e-01 -1.39121878e+00  9.64328408e-01  2.83636838e-01\n",
      "  1.69595048e-01 -7.03859180e-02  1.36531758e+00  5.79247236e-01\n",
      "  1.61784664e-01  3.66050392e-01  1.21139193e+00  1.12916625e+00\n",
      "  1.28601074e-01 -1.72686875e+00 -5.68444014e-01  1.58453786e+00\n",
      "  1.42563665e+00 -1.81520417e-01  8.30320776e-01  7.26246476e-01\n",
      " -1.33609927e+00 -1.79395169e-01  1.09107661e+00  7.88551509e-01\n",
      "  1.90908217e+00 -1.78521603e-01 -1.17402887e+00 -4.63880986e-01\n",
      "  1.01829886e+00  1.48780227e+00  1.32626975e+00 -1.41770697e+00\n",
      " -3.71046305e-01 -1.61386862e-01  4.37001467e-01 -1.55238259e+00\n",
      "  1.32008687e-01 -1.47534084e+00 -6.03327811e-01 -6.53998256e-01\n",
      " -1.71327710e+00 -1.05991542e+00 -1.31817782e+00 -1.02936253e-01\n",
      "  1.39056110e+00 -1.46505558e+00 -1.65992725e+00 -2.02156380e-01\n",
      " -1.70265937e+00  8.53325367e-01  2.30939034e-02 -3.81906301e-01\n",
      " -1.36035824e+00  1.69280350e+00 -1.33492982e+00 -1.06332481e+00\n",
      " -1.68379748e+00  5.76534808e-01 -1.04062390e+00  1.37885940e+00\n",
      " -1.55290282e+00 -5.23342729e-01  5.85997581e-01 -1.03754163e+00\n",
      " -6.13291740e-01  8.12183738e-01 -7.71749556e-01  1.85797882e+00\n",
      "  7.84871578e-01 -1.15188646e+00  5.15375197e-01  7.92385101e-01\n",
      " -2.33245879e-01 -9.67527807e-01 -1.02906078e-01 -1.31459653e+00\n",
      " -6.27725422e-01 -2.01392367e-01 -5.34142911e-01  9.37769055e-01\n",
      "  6.31057858e-01  1.14973414e+00  1.51175809e+00 -6.05908513e-01\n",
      "  1.57201374e+00  3.87419760e-01  1.06707919e+00  1.69182718e+00\n",
      " -8.98935795e-01  1.57348156e+00 -1.32537520e+00 -3.65951896e-01\n",
      "  1.74103117e+00 -3.21406841e-01  2.05081844e+00  9.34672475e-01\n",
      "  1.09501854e-01 -2.69110143e-01  9.39684361e-02 -1.00929940e+00\n",
      "  8.30255270e-01 -1.25341904e+00 -1.70246184e+00 -1.02045691e+00\n",
      " -1.01770961e+00 -2.95078129e-01  1.26511955e+00  9.95545834e-02\n",
      "  7.88262844e-01 -1.16435957e+00 -1.51318741e+00  7.82847822e-01\n",
      " -1.87253189e+00 -2.34842658e+00  2.06603909e+00 -9.91093934e-01\n",
      "  1.37119043e+00 -8.73727381e-01  1.12095094e+00  8.52124989e-01\n",
      "  1.26563597e+00 -2.03031397e+00 -1.91591710e-01  1.38346183e+00\n",
      "  1.46740508e+00 -1.83409750e-01 -1.90685183e-01  1.19608307e+00\n",
      "  1.38042951e+00  1.27460015e+00 -1.21786761e+00 -7.74055645e-02\n",
      "  2.91202188e-01  9.47502613e-01  2.62554973e-01 -1.23693490e+00\n",
      " -6.95223451e-01  2.56094158e-01 -3.68066967e-01 -8.51175040e-02\n",
      "  1.00227094e+00  2.00692728e-01 -1.20041394e+00  3.88659805e-01\n",
      " -1.54674482e+00  1.57349229e+00 -1.62356353e+00  1.40936852e+00\n",
      "  1.04312861e+00 -3.13494392e-02  1.44794679e+00 -4.84503210e-01\n",
      "  9.09968972e-01  3.60662758e-01  4.85693932e-01 -2.16755256e-01\n",
      "  6.57246053e-01  1.28830469e+00  3.75110686e-01 -1.91136718e+00\n",
      "  1.51937604e+00 -1.25857615e+00 -1.69303322e+00  1.73830628e+00\n",
      "  2.33022906e-02 -5.56510806e-01 -1.65453064e+00 -1.12312877e+00\n",
      "  6.35915160e-01  1.73581755e+00 -1.12261608e-01  8.34923565e-01\n",
      "  1.44791257e+00  8.03737044e-01  2.40842611e-01  2.00768277e-01\n",
      " -4.08502668e-01  1.23581834e-04 -1.55465886e-01  3.52390826e-01\n",
      " -3.68638098e-01 -5.32848895e-01 -1.28180468e+00 -1.19081497e-01\n",
      "  1.18914282e+00 -7.75301099e-01 -5.60584724e-01  4.22303051e-01]\n",
      "\n",
      "############################################################\n",
      "scGPT_brain\n",
      "resume model from scGPT_brain/best_model.pt weights, model args override the scGPT_brain/args.json config\n",
      "The size of the vocabulary is 60697\n",
      "Loading parameters encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "Loading parameters encoder.enc_norm.weight with shape torch.Size([512])\n",
      "Loading parameters encoder.enc_norm.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "Loading parameters value_encoder.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters value_encoder.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.weight with shape torch.Size([512])\n",
      "Loading parameters value_encoder.norm.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "Loading parameters transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.0.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "Loading parameters decoder.fc.2.bias with shape torch.Size([512])\n",
      "Loading parameters decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "Loading parameters decoder.fc.4.bias with shape torch.Size([1])\n",
      "\n",
      "Total number of embedding vectors: 60697\n",
      "Length of each vector 512\n",
      "\n",
      "Test Gene: RP5-973N23.5\n",
      "[-1.44472873e+00  7.46650040e-01  1.80740023e+00 -1.43863869e+00\n",
      " -3.47161382e-01  3.78230631e-01  9.00639296e-01  1.44376004e+00\n",
      "  4.28817332e-01 -1.82472885e+00 -1.83873212e+00  1.47242713e+00\n",
      "  1.13047695e+00 -5.75915575e-01  1.79831550e-01  9.22002614e-01\n",
      "  1.77638024e-01  5.93167841e-01  3.09972405e-01 -8.74841630e-01\n",
      "  7.35525846e-01  1.23672032e+00  2.35975489e-01 -6.12671912e-01\n",
      "  2.48664171e-01 -9.26056147e-01  2.14820668e-01 -1.67996585e+00\n",
      "  1.73203075e+00 -4.37952727e-01 -4.58046615e-01  1.63647187e+00\n",
      "  1.44636059e+00  6.77295446e-01 -5.84464014e-01 -3.47669721e-01\n",
      " -5.13447583e-01  1.29043865e+00 -6.57494292e-02 -3.38304937e-01\n",
      "  1.31670642e+00  2.94639885e-01  1.65438965e-01 -1.74345112e+00\n",
      " -1.17831182e+00  1.27652514e+00  1.62376559e+00  8.73071969e-01\n",
      " -1.22757244e+00  8.11951041e-01  3.42658192e-01  8.93301666e-01\n",
      " -2.21928254e-01 -1.86224413e+00  5.49199760e-01 -1.70398962e+00\n",
      " -1.81617665e+00  6.16910696e-01 -1.71541679e+00 -1.84675002e+00\n",
      " -1.17195201e+00  4.02716041e-01  9.39310431e-01 -1.45698643e+00\n",
      " -1.20925248e+00  1.10227323e+00 -9.18675780e-01 -3.68967682e-01\n",
      " -1.51449668e+00 -1.23914552e+00  3.37509960e-02 -1.04479861e+00\n",
      " -5.61402738e-01  8.12876076e-02 -1.15678871e+00 -8.05258393e-01\n",
      " -1.02707422e+00  1.41435230e+00 -1.42104542e+00 -1.55102766e+00\n",
      "  5.27991176e-01  1.85267878e+00 -1.26878238e+00 -2.71702081e-01\n",
      "  4.56426263e-01 -4.41366851e-01  1.05304885e+00 -8.86925638e-01\n",
      "  1.04152608e+00  2.55485803e-01 -1.75425899e+00  4.93177064e-02\n",
      " -1.51977658e+00  1.53281069e+00 -6.50541663e-01 -5.24009109e-01\n",
      " -1.59614146e-01  1.47372156e-01  6.96342528e-01  1.10959339e+00\n",
      "  5.05880415e-01  1.81650496e+00 -8.33447158e-01  1.60257363e+00\n",
      "  1.10680199e+00 -1.12329042e+00  2.18329877e-01  2.79015183e-01\n",
      " -9.11152005e-01 -3.03469986e-01 -1.77810717e+00  2.05358654e-01\n",
      "  2.57072449e-01  1.81652012e-03  1.07151628e+00 -9.33384001e-02\n",
      " -5.11495411e-01  1.41695723e-01 -1.47146964e+00  1.08720779e+00\n",
      " -2.03567600e+00  9.79318678e-01 -1.73425078e-01  9.25040364e-01\n",
      " -3.80202055e-01  1.37509704e+00 -3.48669738e-02 -1.94141805e+00\n",
      " -4.97078121e-01 -8.12066123e-02 -1.09231412e+00 -1.34914339e+00\n",
      "  1.27490449e+00  6.79290354e-01 -4.47616935e-01 -6.78003609e-01\n",
      " -1.54675162e+00  1.12933800e-01 -6.15394413e-01 -8.18004847e-01\n",
      " -6.75503731e-01  5.96182525e-01  1.12332952e+00  1.38580453e+00\n",
      "  1.53136015e+00  9.81647149e-02  6.30322218e-01 -1.56951571e+00\n",
      "  4.88833785e-01  7.04364538e-01  1.26522946e+00  1.26070654e+00\n",
      " -6.43334627e-01 -1.68606067e+00  1.41133797e+00 -1.44093728e+00\n",
      "  1.15472853e+00  1.02983844e+00 -1.94165409e+00 -1.50932372e-01\n",
      " -2.03272438e+00  2.03427458e+00 -3.65320414e-01 -1.60768139e+00\n",
      "  1.65663528e+00  6.99840307e-01 -1.01971269e+00 -1.40321779e+00\n",
      " -1.20942676e+00 -1.75918901e+00  8.81518126e-01  1.41216052e+00\n",
      "  1.30518937e+00  1.45019698e+00 -1.83016741e+00  1.29187822e+00\n",
      "  6.16296470e-01 -1.28478813e+00 -1.30026686e+00  6.62190616e-01\n",
      " -6.93750679e-01  1.30243337e+00  1.22908807e+00 -2.63082147e-01\n",
      " -2.31731445e-01 -1.06967545e+00 -8.09390604e-01 -1.29887342e+00\n",
      " -6.71301365e-01  1.23048306e+00 -1.50413895e+00  5.94324052e-01\n",
      " -8.38339031e-02  1.25348702e-01 -8.00592542e-01  1.18379608e-01\n",
      "  7.27761865e-01  1.52845001e+00 -7.81880379e-01 -1.00165939e+00\n",
      "  7.29253590e-01  2.92156249e-01 -1.05548966e+00 -7.60936499e-01\n",
      " -7.35367298e-01  2.76660509e-02 -9.49648023e-01  1.23409176e+00\n",
      "  1.17560172e+00 -9.79124069e-01 -1.34158587e+00  1.53224063e+00\n",
      "  1.43569899e+00 -1.10100424e+00 -1.31500793e+00 -1.54794708e-01\n",
      " -2.71353841e-01  1.23515517e-01 -1.08540988e+00 -1.46370471e+00\n",
      " -2.14885283e+00  7.77502000e-01  2.47979745e-01  6.48676872e-01\n",
      " -1.34505296e+00 -8.74746799e-01 -1.08181143e+00 -1.66030288e+00\n",
      "  2.39477307e-01 -2.05909204e+00 -5.85168183e-01 -1.32789218e+00\n",
      "  8.36346686e-01 -8.02000165e-01  1.56110644e+00  8.90452385e-01\n",
      " -4.97019976e-01  1.86083913e+00 -1.55962777e+00 -1.02499759e+00\n",
      " -5.80643535e-01 -1.33529913e+00 -1.67893684e+00 -7.25389197e-02\n",
      " -8.38146627e-01  1.84860289e+00  1.23532081e+00  2.89077550e-01\n",
      "  1.56381094e+00  9.02877450e-01  1.49359417e+00 -3.46820056e-01\n",
      " -9.89640713e-01  9.37965930e-01 -1.54533458e+00  7.75428236e-01\n",
      "  1.22586645e-01  1.01471998e-01  6.33714795e-02  1.35481763e+00\n",
      " -1.12613785e+00  1.64036378e-01  9.12445188e-01 -2.65922099e-01\n",
      " -1.58797812e+00  6.18741453e-01 -1.02074027e+00 -6.46412849e-01\n",
      "  9.12523806e-01  1.63797677e+00 -1.25367272e+00 -5.13274729e-01\n",
      " -1.58348644e+00 -1.63361073e+00  1.04513288e+00 -2.96040922e-01\n",
      " -4.96422440e-01  5.95338166e-01  1.58047867e+00  1.19448245e+00\n",
      "  1.43875480e+00  1.06377792e+00 -7.37681508e-01  6.60362422e-01\n",
      "  6.02698505e-01  1.70789313e+00 -1.84983110e+00  7.75466084e-01\n",
      " -8.99413168e-01  1.30641961e+00  1.51676670e-01  1.30681956e+00\n",
      "  8.54556799e-01  3.31631213e-01 -2.40907744e-01  1.26401174e+00\n",
      " -1.21090388e+00  4.64162886e-01  3.07504404e-02  5.67597270e-01\n",
      "  9.55964684e-01 -4.89186794e-01  2.33828485e-01 -1.34225771e-01\n",
      "  1.30133355e+00 -3.15604836e-01  3.43675822e-01 -6.55660629e-01\n",
      "  6.78856730e-01  1.40527070e+00  1.76083967e-01 -7.31309772e-01\n",
      "  3.56451899e-01  5.90822697e-01  7.55028665e-01  5.57992101e-01\n",
      "  7.29756296e-01 -6.65146947e-01 -7.93689609e-01  1.00118542e+00\n",
      "  8.72343659e-01  1.00887191e+00  1.37040877e+00  1.58690953e+00\n",
      " -5.07863283e-01 -1.32668269e+00  9.17655945e-01  1.64222628e-01\n",
      "  1.37768418e-01  1.23730965e-01  1.21580851e+00  5.75591922e-01\n",
      " -1.98358055e-02  1.16075061e-01  1.09556246e+00  7.58959532e-01\n",
      "  3.47829342e-01 -1.31015551e+00 -5.33118963e-01  1.43334973e+00\n",
      "  1.05190122e+00 -1.44439742e-01  9.74856734e-01  5.81598341e-01\n",
      " -1.37360919e+00 -1.52747303e-01  1.02503753e+00  6.44478381e-01\n",
      "  1.66129386e+00 -2.84065962e-01 -1.20885515e+00 -4.34046358e-01\n",
      "  9.58368897e-01  1.84100008e+00  1.28520024e+00 -1.74181736e+00\n",
      " -1.32823929e-01 -2.32147142e-01  3.78278434e-01 -1.58696580e+00\n",
      "  1.18304498e-01 -1.34001017e+00 -5.96351564e-01 -4.64311808e-01\n",
      " -1.72596407e+00 -7.73933053e-01 -1.27103066e+00 -7.77825564e-02\n",
      "  1.34659922e+00 -1.46212399e+00 -1.48809993e+00 -2.27798730e-01\n",
      " -1.60241342e+00  7.92520821e-01  3.75075191e-02 -1.71427310e-01\n",
      " -1.20830333e+00  1.53907013e+00 -1.36444438e+00 -7.82393754e-01\n",
      " -1.46996379e+00  3.59506428e-01 -9.77854908e-01  1.26561904e+00\n",
      " -1.13846004e+00 -5.01195252e-01  6.47119224e-01 -9.79173124e-01\n",
      " -5.51651537e-01  7.89802253e-01 -6.19481921e-01  1.83761787e+00\n",
      "  7.05922902e-01 -1.11288035e+00  5.11071861e-01  7.08230972e-01\n",
      " -2.97107935e-01 -1.02891862e+00 -1.02948211e-01 -1.18145788e+00\n",
      " -5.42137265e-01 -5.93395591e-01 -5.79355836e-01  7.28978336e-01\n",
      "  5.32493889e-01  1.25186539e+00  1.46587360e+00 -6.11414850e-01\n",
      "  1.83559132e+00  6.12376094e-01  1.12445617e+00  1.46604657e+00\n",
      " -9.73362029e-01  1.69527948e+00 -1.31758404e+00 -4.90672767e-01\n",
      "  1.57128227e+00 -5.65917611e-01  1.56201363e+00  1.08639503e+00\n",
      "  9.58133116e-02 -4.11943197e-01  1.71625406e-01 -1.00133109e+00\n",
      "  7.25794077e-01 -1.19792902e+00 -1.43900192e+00 -1.11018419e+00\n",
      " -1.19544923e+00 -2.90294617e-01  8.85026276e-01  9.13003162e-02\n",
      "  5.22475779e-01 -1.12838566e+00 -1.44554019e+00  5.87245822e-01\n",
      " -1.75235033e+00 -1.91679037e+00  2.19005013e+00 -8.29318047e-01\n",
      "  1.30464745e+00 -8.73336613e-01  1.05909669e+00  8.92410517e-01\n",
      "  1.22623014e+00 -1.79338789e+00 -2.70150304e-01  1.24583161e+00\n",
      "  1.61520338e+00  4.26000431e-02 -1.80418611e-01  1.23258901e+00\n",
      "  1.27415287e+00  1.16471958e+00 -1.17700136e+00 -1.12855405e-01\n",
      "  3.07161629e-01  8.52663040e-01  2.44539410e-01 -1.25025523e+00\n",
      " -8.96232784e-01  3.87683690e-01 -2.19080254e-01 -6.25680238e-02\n",
      "  1.50770557e+00  1.78563669e-01 -1.28010225e+00  2.46641085e-01\n",
      " -1.35164845e+00  1.60078430e+00 -1.72435033e+00  1.34139407e+00\n",
      "  9.99410689e-01  5.11111580e-02  1.34750497e+00 -6.57106578e-01\n",
      "  8.68846774e-01  4.81287628e-01  4.75377440e-01 -1.49507940e-01\n",
      "  6.39443278e-01  1.30249763e+00  4.44037259e-01 -1.76347363e+00\n",
      "  1.14966643e+00 -1.08483827e+00 -1.63375998e+00  1.63297081e+00\n",
      "  1.01044603e-01 -9.36392128e-01 -1.74028826e+00 -9.36269104e-01\n",
      "  2.90604472e-01  1.63942587e+00  1.14216907e-02  9.14424419e-01\n",
      "  1.26975775e+00  7.83200085e-01  3.42891723e-01  4.13370281e-01\n",
      " -1.32001221e-01 -3.74579877e-02 -1.29268244e-01  3.54530573e-01\n",
      " -4.35065329e-01 -5.72614193e-01 -1.03768456e+00 -6.40214905e-02\n",
      "  1.12821627e+00 -6.97566748e-01 -4.42692488e-01  4.29959625e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models=[f for f in  os.listdir('.') if f.startswith('scGPT_')]\n",
    "\n",
    "for model in models:\n",
    "    output_pkl = './scgpt_embeddings/' + model + '_embeddings.pkl'\n",
    "    print('#' * 60)\n",
    "    print(model)\n",
    "    export2embeddings(model, output_pkl)\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt_conda 3.9",
   "language": "python",
   "name": "scgpt_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
